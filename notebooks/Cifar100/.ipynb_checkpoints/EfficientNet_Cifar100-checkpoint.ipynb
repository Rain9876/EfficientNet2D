{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "The87On52M1Q",
    "outputId": "b110b133-616f-4e7e-ffde-2f848ea96fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geffnet in /usr/local/lib/python3.6/dist-packages (0.9.7)\n",
      "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from geffnet) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from geffnet) (0.4.0a0+d31eafa)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (6.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (1.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torchsummary\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting timm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e9/dfee5381ae8e7862d8565cfc9ad7056dccbf2eefa214256da6b2fd878702/timm-0.1.18-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm) (0.4.0a0+d31eafa)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (6.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (1.17.0)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.1.18\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install geffnet\n",
    "!pip install torchsummary\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THFFFLtvqxwp"
   },
   "outputs": [],
   "source": [
    "import geffnet\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "from timm.optim.radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Y_xGDxtxqyGp",
    "outputId": "7c43f23a-59a9-4dfd-df53-5eec207cc01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "use_GPU = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
    "if use_GPU:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    print('Device: ' + str(device))\n",
    "    print('GPU: ' + str(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"Using GPU: {}\".format(use_GPU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yrPQf4Sq0sI"
   },
   "outputs": [],
   "source": [
    "def ImageProcessing():\n",
    "    transform = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "\n",
    "    train_dat = datasets.CIFAR100(root=sys.path[0] + \"/data/CIFAR100\", train=True, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dat, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    test_dat = datasets.CIFAR100(root=sys.path[0] + '/data/CIFAR100', train=False, download=True, transform=transform)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dat, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9i2kRq9Rq5gn"
   },
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion, epoch):\n",
    "    training_loss = 0\n",
    "    model.train()\n",
    "    bi = 200\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        if batch_idx % bi == bi-1:  # print every 2000 mini-batches\n",
    "            print('[epoch: %d, batch: %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, training_loss / bi))\n",
    "            training_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7USv_wJ1q50m"
   },
   "outputs": [],
   "source": [
    "def testing(model, test_loader, criterion):\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, targets in test_loader:\n",
    "          \n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            total += 1\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {:.4f}\\n\".format(test_loss / total,\n",
    "                                                                        correct / len(test_loader.dataset)))\n",
    "    return test_loss / total, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkhUTENfq_D1"
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLK9bp07q_Xd"
   },
   "outputs": [],
   "source": [
    "def fine_tuning(model):\n",
    "\n",
    "    epochs = 20\n",
    "\n",
    "    test_acc = []\n",
    "    test_loss = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # optimizer = torch.optim.RMSprop(params_to_update, lr = 0.0001, weight_decay =1e-5)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.99, weight_decay=1e-5)\n",
    "    # optimizer = torch.optim.SGD(params_to_update, lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
    "\n",
    "    train_loader, test_loader = ImageProcessing()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print_learning_rate(optimizer, epoch+1)\n",
    "\n",
    "        training(model, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "        loss, acc = testing(model, test_loader, criterion)\n",
    "\n",
    "        test_loss.append(loss)\n",
    "\n",
    "        test_acc.append(acc)\n",
    "\n",
    "        reduce_lr.step(loss)\n",
    "\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    ax1.plot(list(range(1,epochs+1)), test_acc)\n",
    "    ax1.set(xlabel='epochs', ylabel='test accuracy')\n",
    "    ax2.plot(list(range(1,epochs+1)), test_loss)\n",
    "    ax2.set(xlabel='epochs', ylabel='test loss')\n",
    "    fig.tight_layout(pad=4.0)\n",
    "    ax2.set_xticks(np.arange(1, epochs+1, step=1))\n",
    "    ax1.set_xticks(np.arange(1, epochs+1, step=1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J460uqKKrG4p"
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(lr, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 0.97 every 2.4 epochs\"\"\"\n",
    "    lr = lr * (0.97 ** (epoch // 2))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def print_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Epoch: [{}] Current learning rate (lr) = {}\".format(\n",
    "                                                    epoch, param_group['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZhSJGnMrDsU"
   },
   "outputs": [],
   "source": [
    "def main(md):\n",
    "\n",
    "    # classes = ('plane', 'car', 'bird', 'cat',\n",
    "    #            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    # classes = (\n",
    "    # 'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    # 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    # 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    # 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    # 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    # 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    # 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    # 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    # 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    # 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    # 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    # 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    # 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    # 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    # 'worm')\n",
    "\n",
    "    train_loader, test_loader = ImageProcessing()\n",
    "    \n",
    "    # print(len(classes))\n",
    "    # imageshow(train_loader, classes)\n",
    "\n",
    "    momentum = 0.9\n",
    "    epochs = 40\n",
    "    decay = 1e-5\n",
    "    lr = 0.001\n",
    "\n",
    "    model = md.to(device)\n",
    "\n",
    "    summary(model,(3,224,224),batch_size=10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "    \n",
    "    optimizer = RAdam(model.parameters(), lr=lr, weight_decay = decay)\n",
    "\n",
    "    # reduce_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.6, patience=1)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print_learning_rate(optimizer,epoch+1)\n",
    "        training(model, train_loader, optimizer, criterion, epoch)\n",
    "        loss,_ = testing(model, test_loader, criterion)\n",
    "        # adjust_learning_rate(lr, optimizer,epoch+1)\n",
    "        reduce_lr.step(loss)\n",
    "\n",
    "    # PATH = './cifar_net.pth'\n",
    "    # torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "fe4738ef0bfe46ee8677a43ccf013b7f",
      "16e844c36acb49968c4658b84c6791c6",
      "8864d4f7405341a6be208d132866e659",
      "e92b75192404427180b16ac807e47d8b",
      "56c6650711ee4f95adcfd244d0a9d165",
      "6d46c0f9493f4d7289f38e6a8ed158ed",
      "01f474c3e87b4742a43aa43bd306cb31",
      "c6e3d437239549b69f47e2e45b0f2442"
     ]
    },
    "colab_type": "code",
    "id": "ktT9bF7krEES",
    "outputId": "89dff731-fdfe-4d8e-cdbd-4a93b9887fc9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_es', 'fbnetc_100', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_a1', 'mnasnet_b1', 'mobilenetv3_rw', 'spnasnet_100', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [10, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [10, 32, 112, 112]              64\n",
      "          SwishJit-3         [10, 32, 112, 112]               0\n",
      "            Conv2d-4         [10, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [10, 32, 112, 112]              64\n",
      "          SwishJit-6         [10, 32, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [10, 32, 1, 1]               0\n",
      "            Conv2d-8              [10, 8, 1, 1]             264\n",
      "          SwishJit-9              [10, 8, 1, 1]               0\n",
      "           Conv2d-10             [10, 32, 1, 1]             288\n",
      "    SqueezeExcite-11         [10, 32, 112, 112]               0\n",
      "           Conv2d-12         [10, 16, 112, 112]             512\n",
      "      BatchNorm2d-13         [10, 16, 112, 112]              32\n",
      "         Identity-14         [10, 16, 112, 112]               0\n",
      "DepthwiseSeparableConv-15         [10, 16, 112, 112]               0\n",
      "           Conv2d-16         [10, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-17         [10, 96, 112, 112]             192\n",
      "         SwishJit-18         [10, 96, 112, 112]               0\n",
      "           Conv2d-19           [10, 96, 56, 56]             864\n",
      "      BatchNorm2d-20           [10, 96, 56, 56]             192\n",
      "         SwishJit-21           [10, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-22             [10, 96, 1, 1]               0\n",
      "           Conv2d-23              [10, 4, 1, 1]             388\n",
      "         SwishJit-24              [10, 4, 1, 1]               0\n",
      "           Conv2d-25             [10, 96, 1, 1]             480\n",
      "    SqueezeExcite-26           [10, 96, 56, 56]               0\n",
      "           Conv2d-27           [10, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-28           [10, 24, 56, 56]              48\n",
      " InvertedResidual-29           [10, 24, 56, 56]               0\n",
      "           Conv2d-30          [10, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-31          [10, 144, 56, 56]             288\n",
      "         SwishJit-32          [10, 144, 56, 56]               0\n",
      "           Conv2d-33          [10, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-34          [10, 144, 56, 56]             288\n",
      "         SwishJit-35          [10, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-36            [10, 144, 1, 1]               0\n",
      "           Conv2d-37              [10, 6, 1, 1]             870\n",
      "         SwishJit-38              [10, 6, 1, 1]               0\n",
      "           Conv2d-39            [10, 144, 1, 1]           1,008\n",
      "    SqueezeExcite-40          [10, 144, 56, 56]               0\n",
      "           Conv2d-41           [10, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-42           [10, 24, 56, 56]              48\n",
      " InvertedResidual-43           [10, 24, 56, 56]               0\n",
      "           Conv2d-44          [10, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-45          [10, 144, 56, 56]             288\n",
      "         SwishJit-46          [10, 144, 56, 56]               0\n",
      "           Conv2d-47          [10, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-48          [10, 144, 28, 28]             288\n",
      "         SwishJit-49          [10, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-50            [10, 144, 1, 1]               0\n",
      "           Conv2d-51              [10, 6, 1, 1]             870\n",
      "         SwishJit-52              [10, 6, 1, 1]               0\n",
      "           Conv2d-53            [10, 144, 1, 1]           1,008\n",
      "    SqueezeExcite-54          [10, 144, 28, 28]               0\n",
      "           Conv2d-55           [10, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-56           [10, 40, 28, 28]              80\n",
      " InvertedResidual-57           [10, 40, 28, 28]               0\n",
      "           Conv2d-58          [10, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-59          [10, 240, 28, 28]             480\n",
      "         SwishJit-60          [10, 240, 28, 28]               0\n",
      "           Conv2d-61          [10, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-62          [10, 240, 28, 28]             480\n",
      "         SwishJit-63          [10, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-64            [10, 240, 1, 1]               0\n",
      "           Conv2d-65             [10, 10, 1, 1]           2,410\n",
      "         SwishJit-66             [10, 10, 1, 1]               0\n",
      "           Conv2d-67            [10, 240, 1, 1]           2,640\n",
      "    SqueezeExcite-68          [10, 240, 28, 28]               0\n",
      "           Conv2d-69           [10, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-70           [10, 40, 28, 28]              80\n",
      " InvertedResidual-71           [10, 40, 28, 28]               0\n",
      "           Conv2d-72          [10, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-73          [10, 240, 28, 28]             480\n",
      "         SwishJit-74          [10, 240, 28, 28]               0\n",
      "           Conv2d-75          [10, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-76          [10, 240, 14, 14]             480\n",
      "         SwishJit-77          [10, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-78            [10, 240, 1, 1]               0\n",
      "           Conv2d-79             [10, 10, 1, 1]           2,410\n",
      "         SwishJit-80             [10, 10, 1, 1]               0\n",
      "           Conv2d-81            [10, 240, 1, 1]           2,640\n",
      "    SqueezeExcite-82          [10, 240, 14, 14]               0\n",
      "           Conv2d-83           [10, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-84           [10, 80, 14, 14]             160\n",
      " InvertedResidual-85           [10, 80, 14, 14]               0\n",
      "           Conv2d-86          [10, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-87          [10, 480, 14, 14]             960\n",
      "         SwishJit-88          [10, 480, 14, 14]               0\n",
      "           Conv2d-89          [10, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-90          [10, 480, 14, 14]             960\n",
      "         SwishJit-91          [10, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-92            [10, 480, 1, 1]               0\n",
      "           Conv2d-93             [10, 20, 1, 1]           9,620\n",
      "         SwishJit-94             [10, 20, 1, 1]               0\n",
      "           Conv2d-95            [10, 480, 1, 1]          10,080\n",
      "    SqueezeExcite-96          [10, 480, 14, 14]               0\n",
      "           Conv2d-97           [10, 80, 14, 14]          38,400\n",
      "      BatchNorm2d-98           [10, 80, 14, 14]             160\n",
      " InvertedResidual-99           [10, 80, 14, 14]               0\n",
      "          Conv2d-100          [10, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-101          [10, 480, 14, 14]             960\n",
      "        SwishJit-102          [10, 480, 14, 14]               0\n",
      "          Conv2d-103          [10, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-104          [10, 480, 14, 14]             960\n",
      "        SwishJit-105          [10, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-106            [10, 480, 1, 1]               0\n",
      "          Conv2d-107             [10, 20, 1, 1]           9,620\n",
      "        SwishJit-108             [10, 20, 1, 1]               0\n",
      "          Conv2d-109            [10, 480, 1, 1]          10,080\n",
      "   SqueezeExcite-110          [10, 480, 14, 14]               0\n",
      "          Conv2d-111           [10, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-112           [10, 80, 14, 14]             160\n",
      "InvertedResidual-113           [10, 80, 14, 14]               0\n",
      "          Conv2d-114          [10, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-115          [10, 480, 14, 14]             960\n",
      "        SwishJit-116          [10, 480, 14, 14]               0\n",
      "          Conv2d-117          [10, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-118          [10, 480, 14, 14]             960\n",
      "        SwishJit-119          [10, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-120            [10, 480, 1, 1]               0\n",
      "          Conv2d-121             [10, 20, 1, 1]           9,620\n",
      "        SwishJit-122             [10, 20, 1, 1]               0\n",
      "          Conv2d-123            [10, 480, 1, 1]          10,080\n",
      "   SqueezeExcite-124          [10, 480, 14, 14]               0\n",
      "          Conv2d-125          [10, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-126          [10, 112, 14, 14]             224\n",
      "InvertedResidual-127          [10, 112, 14, 14]               0\n",
      "          Conv2d-128          [10, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-129          [10, 672, 14, 14]           1,344\n",
      "        SwishJit-130          [10, 672, 14, 14]               0\n",
      "          Conv2d-131          [10, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-132          [10, 672, 14, 14]           1,344\n",
      "        SwishJit-133          [10, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-134            [10, 672, 1, 1]               0\n",
      "          Conv2d-135             [10, 28, 1, 1]          18,844\n",
      "        SwishJit-136             [10, 28, 1, 1]               0\n",
      "          Conv2d-137            [10, 672, 1, 1]          19,488\n",
      "   SqueezeExcite-138          [10, 672, 14, 14]               0\n",
      "          Conv2d-139          [10, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-140          [10, 112, 14, 14]             224\n",
      "InvertedResidual-141          [10, 112, 14, 14]               0\n",
      "          Conv2d-142          [10, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-143          [10, 672, 14, 14]           1,344\n",
      "        SwishJit-144          [10, 672, 14, 14]               0\n",
      "          Conv2d-145          [10, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-146          [10, 672, 14, 14]           1,344\n",
      "        SwishJit-147          [10, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-148            [10, 672, 1, 1]               0\n",
      "          Conv2d-149             [10, 28, 1, 1]          18,844\n",
      "        SwishJit-150             [10, 28, 1, 1]               0\n",
      "          Conv2d-151            [10, 672, 1, 1]          19,488\n",
      "   SqueezeExcite-152          [10, 672, 14, 14]               0\n",
      "          Conv2d-153          [10, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-154          [10, 112, 14, 14]             224\n",
      "InvertedResidual-155          [10, 112, 14, 14]               0\n",
      "          Conv2d-156          [10, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-157          [10, 672, 14, 14]           1,344\n",
      "        SwishJit-158          [10, 672, 14, 14]               0\n",
      "          Conv2d-159            [10, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-160            [10, 672, 7, 7]           1,344\n",
      "        SwishJit-161            [10, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-162            [10, 672, 1, 1]               0\n",
      "          Conv2d-163             [10, 28, 1, 1]          18,844\n",
      "        SwishJit-164             [10, 28, 1, 1]               0\n",
      "          Conv2d-165            [10, 672, 1, 1]          19,488\n",
      "   SqueezeExcite-166            [10, 672, 7, 7]               0\n",
      "          Conv2d-167            [10, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-168            [10, 192, 7, 7]             384\n",
      "InvertedResidual-169            [10, 192, 7, 7]               0\n",
      "          Conv2d-170           [10, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-171           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-172           [10, 1152, 7, 7]               0\n",
      "          Conv2d-173           [10, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-174           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-175           [10, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-176           [10, 1152, 1, 1]               0\n",
      "          Conv2d-177             [10, 48, 1, 1]          55,344\n",
      "        SwishJit-178             [10, 48, 1, 1]               0\n",
      "          Conv2d-179           [10, 1152, 1, 1]          56,448\n",
      "   SqueezeExcite-180           [10, 1152, 7, 7]               0\n",
      "          Conv2d-181            [10, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-182            [10, 192, 7, 7]             384\n",
      "InvertedResidual-183            [10, 192, 7, 7]               0\n",
      "          Conv2d-184           [10, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-185           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-186           [10, 1152, 7, 7]               0\n",
      "          Conv2d-187           [10, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-188           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-189           [10, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-190           [10, 1152, 1, 1]               0\n",
      "          Conv2d-191             [10, 48, 1, 1]          55,344\n",
      "        SwishJit-192             [10, 48, 1, 1]               0\n",
      "          Conv2d-193           [10, 1152, 1, 1]          56,448\n",
      "   SqueezeExcite-194           [10, 1152, 7, 7]               0\n",
      "          Conv2d-195            [10, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-196            [10, 192, 7, 7]             384\n",
      "InvertedResidual-197            [10, 192, 7, 7]               0\n",
      "          Conv2d-198           [10, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-199           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-200           [10, 1152, 7, 7]               0\n",
      "          Conv2d-201           [10, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-202           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-203           [10, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-204           [10, 1152, 1, 1]               0\n",
      "          Conv2d-205             [10, 48, 1, 1]          55,344\n",
      "        SwishJit-206             [10, 48, 1, 1]               0\n",
      "          Conv2d-207           [10, 1152, 1, 1]          56,448\n",
      "   SqueezeExcite-208           [10, 1152, 7, 7]               0\n",
      "          Conv2d-209            [10, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-210            [10, 192, 7, 7]             384\n",
      "InvertedResidual-211            [10, 192, 7, 7]               0\n",
      "          Conv2d-212           [10, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-213           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-214           [10, 1152, 7, 7]               0\n",
      "          Conv2d-215           [10, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-216           [10, 1152, 7, 7]           2,304\n",
      "        SwishJit-217           [10, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-218           [10, 1152, 1, 1]               0\n",
      "          Conv2d-219             [10, 48, 1, 1]          55,344\n",
      "        SwishJit-220             [10, 48, 1, 1]               0\n",
      "          Conv2d-221           [10, 1152, 1, 1]          56,448\n",
      "   SqueezeExcite-222           [10, 1152, 7, 7]               0\n",
      "          Conv2d-223            [10, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-224            [10, 320, 7, 7]             640\n",
      "InvertedResidual-225            [10, 320, 7, 7]               0\n",
      "          Conv2d-226           [10, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-227           [10, 1280, 7, 7]           2,560\n",
      "        SwishJit-228           [10, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-229           [10, 1280, 1, 1]               0\n",
      "          Linear-230                  [10, 100]         128,000\n",
      "================================================================\n",
      "Total params: 4,135,548\n",
      "Trainable params: 4,135,548\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5.74\n",
      "Forward/backward pass size (MB): 1734.95\n",
      "Params size (MB): 15.78\n",
      "Estimated Total Size (MB): 1756.47\n",
      "----------------------------------------------------------------\n",
      "Epoch: [1] Current learning rate (lr) = 0.001\n",
      "[epoch: 1, batch:   200] loss: 4.472\n",
      "[epoch: 1, batch:   400] loss: 4.092\n",
      "[epoch: 1, batch:   600] loss: 3.915\n",
      "[epoch: 1, batch:   800] loss: 3.761\n",
      "[epoch: 1, batch:  1000] loss: 3.560\n",
      "[epoch: 1, batch:  1200] loss: 3.415\n",
      "[epoch: 1, batch:  1400] loss: 3.284\n",
      "\n",
      "Test set: Average loss: 3.1322, Accuracy: 0.2099\n",
      "\n",
      "Epoch: [2] Current learning rate (lr) = 0.001\n",
      "[epoch: 2, batch:   200] loss: 3.049\n",
      "[epoch: 2, batch:   400] loss: 2.896\n",
      "[epoch: 2, batch:   600] loss: 2.797\n",
      "[epoch: 2, batch:   800] loss: 2.732\n",
      "[epoch: 2, batch:  1000] loss: 2.567\n",
      "[epoch: 2, batch:  1200] loss: 2.502\n",
      "[epoch: 2, batch:  1400] loss: 2.428\n",
      "\n",
      "Test set: Average loss: 2.3186, Accuracy: 0.3776\n",
      "\n",
      "Epoch: [3] Current learning rate (lr) = 0.001\n",
      "[epoch: 3, batch:   200] loss: 2.299\n",
      "[epoch: 3, batch:   400] loss: 2.164\n",
      "[epoch: 3, batch:   600] loss: 2.159\n",
      "[epoch: 3, batch:   800] loss: 2.108\n",
      "[epoch: 3, batch:  1000] loss: 2.006\n",
      "[epoch: 3, batch:  1200] loss: 1.986\n",
      "[epoch: 3, batch:  1400] loss: 1.931\n",
      "\n",
      "Test set: Average loss: 2.0042, Accuracy: 0.4468\n",
      "\n",
      "Epoch: [4] Current learning rate (lr) = 0.001\n",
      "[epoch: 4, batch:   200] loss: 1.858\n",
      "[epoch: 4, batch:   400] loss: 1.760\n",
      "[epoch: 4, batch:   600] loss: 1.751\n",
      "[epoch: 4, batch:   800] loss: 1.710\n",
      "[epoch: 4, batch:  1000] loss: 1.644\n",
      "[epoch: 4, batch:  1200] loss: 1.643\n",
      "[epoch: 4, batch:  1400] loss: 1.582\n",
      "\n",
      "Test set: Average loss: 1.8962, Accuracy: 0.4784\n",
      "\n",
      "Epoch: [5] Current learning rate (lr) = 0.001\n",
      "[epoch: 5, batch:   200] loss: 1.527\n",
      "[epoch: 5, batch:   400] loss: 1.452\n",
      "[epoch: 5, batch:   600] loss: 1.422\n",
      "[epoch: 5, batch:   800] loss: 1.411\n",
      "[epoch: 5, batch:  1000] loss: 1.357\n",
      "[epoch: 5, batch:  1200] loss: 1.357\n",
      "[epoch: 5, batch:  1400] loss: 1.285\n",
      "\n",
      "Test set: Average loss: 1.8253, Accuracy: 0.5142\n",
      "\n",
      "Epoch: [6] Current learning rate (lr) = 0.001\n",
      "[epoch: 6, batch:   200] loss: 1.229\n",
      "[epoch: 6, batch:   400] loss: 1.174\n",
      "[epoch: 6, batch:   600] loss: 1.146\n",
      "[epoch: 6, batch:   800] loss: 1.141\n",
      "[epoch: 6, batch:  1000] loss: 1.097\n",
      "[epoch: 6, batch:  1200] loss: 1.114\n",
      "[epoch: 6, batch:  1400] loss: 1.037\n",
      "\n",
      "Test set: Average loss: 1.8772, Accuracy: 0.5351\n",
      "\n",
      "Epoch: [7] Current learning rate (lr) = 0.001\n",
      "[epoch: 7, batch:   200] loss: 0.996\n",
      "[epoch: 7, batch:   400] loss: 0.943\n",
      "[epoch: 7, batch:   600] loss: 0.909\n",
      "[epoch: 7, batch:   800] loss: 0.917\n",
      "[epoch: 7, batch:  1000] loss: 0.882\n",
      "[epoch: 7, batch:  1200] loss: 0.898\n",
      "[epoch: 7, batch:  1400] loss: 0.837\n",
      "\n",
      "Test set: Average loss: 1.9564, Accuracy: 0.5366\n",
      "\n",
      "Epoch: [8] Current learning rate (lr) = 0.0006\n",
      "[epoch: 8, batch:   200] loss: 0.757\n",
      "[epoch: 8, batch:   400] loss: 0.654\n",
      "[epoch: 8, batch:   600] loss: 0.635\n",
      "[epoch: 8, batch:   800] loss: 0.583\n",
      "[epoch: 8, batch:  1000] loss: 0.530\n",
      "[epoch: 8, batch:  1200] loss: 0.528\n",
      "[epoch: 8, batch:  1400] loss: 0.449\n",
      "\n",
      "Test set: Average loss: 1.8206, Accuracy: 0.5805\n",
      "\n",
      "Epoch: [9] Current learning rate (lr) = 0.0006\n",
      "[epoch: 9, batch:   200] loss: 0.458\n",
      "[epoch: 9, batch:   400] loss: 0.369\n",
      "[epoch: 9, batch:   600] loss: 0.360\n",
      "[epoch: 9, batch:   800] loss: 0.346\n",
      "[epoch: 9, batch:  1000] loss: 0.313\n",
      "[epoch: 9, batch:  1200] loss: 0.320\n",
      "[epoch: 9, batch:  1400] loss: 0.300\n",
      "\n",
      "Test set: Average loss: 2.0759, Accuracy: 0.5731\n",
      "\n",
      "Epoch: [10] Current learning rate (lr) = 0.0006\n",
      "[epoch: 10, batch:   200] loss: 0.307\n",
      "[epoch: 10, batch:   400] loss: 0.254\n",
      "[epoch: 10, batch:   600] loss: 0.276\n",
      "[epoch: 10, batch:   800] loss: 0.290\n",
      "[epoch: 10, batch:  1000] loss: 0.270\n",
      "[epoch: 10, batch:  1200] loss: 0.268\n",
      "[epoch: 10, batch:  1400] loss: 0.245\n",
      "\n",
      "Test set: Average loss: 2.2242, Accuracy: 0.5780\n",
      "\n",
      "Epoch: [11] Current learning rate (lr) = 0.00035999999999999997\n",
      "[epoch: 11, batch:   200] loss: 0.227\n",
      "[epoch: 11, batch:   400] loss: 0.166\n",
      "[epoch: 11, batch:   600] loss: 0.170\n",
      "[epoch: 11, batch:   800] loss: 0.146\n",
      "[epoch: 11, batch:  1000] loss: 0.138\n",
      "[epoch: 11, batch:  1200] loss: 0.119\n",
      "[epoch: 11, batch:  1400] loss: 0.110\n",
      "\n",
      "Test set: Average loss: 2.1679, Accuracy: 0.6016\n",
      "\n",
      "Epoch: [12] Current learning rate (lr) = 0.00035999999999999997\n",
      "[epoch: 12, batch:   200] loss: 0.096\n",
      "[epoch: 12, batch:   400] loss: 0.079\n",
      "[epoch: 12, batch:   600] loss: 0.067\n",
      "[epoch: 12, batch:   800] loss: 0.068\n",
      "[epoch: 12, batch:  1000] loss: 0.065\n",
      "[epoch: 12, batch:  1200] loss: 0.055\n",
      "[epoch: 12, batch:  1400] loss: 0.056\n",
      "\n",
      "Test set: Average loss: 2.3464, Accuracy: 0.6039\n",
      "\n",
      "Epoch: [13] Current learning rate (lr) = 0.00021599999999999996\n",
      "[epoch: 13, batch:   200] loss: 0.054\n",
      "[epoch: 13, batch:   400] loss: 0.046\n",
      "[epoch: 13, batch:   600] loss: 0.042\n",
      "[epoch: 13, batch:   800] loss: 0.037\n",
      "[epoch: 13, batch:  1000] loss: 0.029\n",
      "[epoch: 13, batch:  1200] loss: 0.028\n",
      "[epoch: 13, batch:  1400] loss: 0.026\n",
      "\n",
      "Test set: Average loss: 2.3003, Accuracy: 0.6209\n",
      "\n",
      "Epoch: [14] Current learning rate (lr) = 0.00021599999999999996\n",
      "[epoch: 14, batch:   200] loss: 0.022\n",
      "[epoch: 14, batch:   400] loss: 0.019\n",
      "[epoch: 14, batch:   600] loss: 0.019\n",
      "[epoch: 14, batch:   800] loss: 0.019\n",
      "[epoch: 14, batch:  1000] loss: 0.014\n",
      "[epoch: 14, batch:  1200] loss: 0.014\n",
      "[epoch: 14, batch:  1400] loss: 0.010\n",
      "\n",
      "Test set: Average loss: 2.3888, Accuracy: 0.6247\n",
      "\n",
      "Epoch: [15] Current learning rate (lr) = 0.00012959999999999998\n",
      "[epoch: 15, batch:   200] loss: 0.011\n",
      "[epoch: 15, batch:   400] loss: 0.010\n",
      "[epoch: 15, batch:   600] loss: 0.009\n",
      "[epoch: 15, batch:   800] loss: 0.007\n",
      "[epoch: 15, batch:  1000] loss: 0.008\n",
      "[epoch: 15, batch:  1200] loss: 0.006\n",
      "[epoch: 15, batch:  1400] loss: 0.005\n",
      "\n",
      "Test set: Average loss: 2.4087, Accuracy: 0.6296\n",
      "\n",
      "Epoch: [16] Current learning rate (lr) = 0.00012959999999999998\n",
      "[epoch: 16, batch:   200] loss: 0.005\n",
      "[epoch: 16, batch:   400] loss: 0.004\n",
      "[epoch: 16, batch:   600] loss: 0.005\n",
      "[epoch: 16, batch:   800] loss: 0.003\n",
      "[epoch: 16, batch:  1000] loss: 0.006\n",
      "[epoch: 16, batch:  1200] loss: 0.006\n",
      "[epoch: 16, batch:  1400] loss: 0.003\n",
      "\n",
      "Test set: Average loss: 2.4835, Accuracy: 0.6294\n",
      "\n",
      "Epoch: [17] Current learning rate (lr) = 7.775999999999999e-05\n",
      "[epoch: 17, batch:   200] loss: 0.004\n",
      "[epoch: 17, batch:   400] loss: 0.004\n",
      "[epoch: 17, batch:   600] loss: 0.002\n",
      "[epoch: 17, batch:   800] loss: 0.002\n",
      "[epoch: 17, batch:  1000] loss: 0.002\n",
      "[epoch: 17, batch:  1200] loss: 0.001\n",
      "[epoch: 17, batch:  1400] loss: 0.001\n",
      "\n",
      "Test set: Average loss: 2.5155, Accuracy: 0.6348\n",
      "\n",
      "Epoch: [18] Current learning rate (lr) = 7.775999999999999e-05\n",
      "[epoch: 18, batch:   200] loss: 0.001\n",
      "[epoch: 18, batch:   400] loss: 0.001\n",
      "[epoch: 18, batch:   600] loss: 0.004\n",
      "[epoch: 18, batch:   800] loss: 0.001\n",
      "[epoch: 18, batch:  1000] loss: 0.006\n",
      "[epoch: 18, batch:  1200] loss: 0.002\n",
      "[epoch: 18, batch:  1400] loss: 0.001\n",
      "\n",
      "Test set: Average loss: 2.5757, Accuracy: 0.6319\n",
      "\n",
      "Epoch: [19] Current learning rate (lr) = 4.665599999999999e-05\n",
      "[epoch: 19, batch:   200] loss: 0.001\n",
      "[epoch: 19, batch:   400] loss: 0.001\n",
      "[epoch: 19, batch:   600] loss: 0.002\n",
      "[epoch: 19, batch:   800] loss: 0.001\n",
      "[epoch: 19, batch:  1000] loss: 0.001\n",
      "[epoch: 19, batch:  1200] loss: 0.001\n",
      "[epoch: 19, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.6109, Accuracy: 0.6355\n",
      "\n",
      "Epoch: [20] Current learning rate (lr) = 4.665599999999999e-05\n",
      "[epoch: 20, batch:   200] loss: 0.001\n",
      "[epoch: 20, batch:   400] loss: 0.000\n",
      "[epoch: 20, batch:   600] loss: 0.001\n",
      "[epoch: 20, batch:   800] loss: 0.000\n",
      "[epoch: 20, batch:  1000] loss: 0.001\n",
      "[epoch: 20, batch:  1200] loss: 0.000\n",
      "[epoch: 20, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.6773, Accuracy: 0.6351\n",
      "\n",
      "Epoch: [21] Current learning rate (lr) = 2.7993599999999992e-05\n",
      "[epoch: 21, batch:   200] loss: 0.000\n",
      "[epoch: 21, batch:   400] loss: 0.000\n",
      "[epoch: 21, batch:   600] loss: 0.000\n",
      "[epoch: 21, batch:   800] loss: 0.000\n",
      "[epoch: 21, batch:  1000] loss: 0.000\n",
      "[epoch: 21, batch:  1200] loss: 0.000\n",
      "[epoch: 21, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.7224, Accuracy: 0.6365\n",
      "\n",
      "Epoch: [22] Current learning rate (lr) = 2.7993599999999992e-05\n",
      "[epoch: 22, batch:   200] loss: 0.000\n",
      "[epoch: 22, batch:   400] loss: 0.000\n",
      "[epoch: 22, batch:   600] loss: 0.000\n",
      "[epoch: 22, batch:   800] loss: 0.000\n",
      "[epoch: 22, batch:  1000] loss: 0.000\n",
      "[epoch: 22, batch:  1200] loss: 0.000\n",
      "[epoch: 22, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.7915, Accuracy: 0.6362\n",
      "\n",
      "Epoch: [23] Current learning rate (lr) = 1.6796159999999994e-05\n",
      "[epoch: 23, batch:   200] loss: 0.000\n",
      "[epoch: 23, batch:   400] loss: 0.000\n",
      "[epoch: 23, batch:   600] loss: 0.000\n",
      "[epoch: 23, batch:   800] loss: 0.000\n",
      "[epoch: 23, batch:  1000] loss: 0.000\n",
      "[epoch: 23, batch:  1200] loss: 0.000\n",
      "[epoch: 23, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.8445, Accuracy: 0.6370\n",
      "\n",
      "Epoch: [24] Current learning rate (lr) = 1.6796159999999994e-05\n",
      "[epoch: 24, batch:   200] loss: 0.000\n",
      "[epoch: 24, batch:   400] loss: 0.000\n",
      "[epoch: 24, batch:   600] loss: 0.000\n",
      "[epoch: 24, batch:   800] loss: 0.000\n",
      "[epoch: 24, batch:  1000] loss: 0.000\n",
      "[epoch: 24, batch:  1200] loss: 0.000\n",
      "[epoch: 24, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.9122, Accuracy: 0.6377\n",
      "\n",
      "Epoch: [25] Current learning rate (lr) = 1.0077695999999996e-05\n",
      "[epoch: 25, batch:   200] loss: 0.000\n",
      "[epoch: 25, batch:   400] loss: 0.000\n",
      "[epoch: 25, batch:   600] loss: 0.000\n",
      "[epoch: 25, batch:   800] loss: 0.000\n",
      "[epoch: 25, batch:  1000] loss: 0.000\n",
      "[epoch: 25, batch:  1200] loss: 0.000\n",
      "[epoch: 25, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 2.9587, Accuracy: 0.6377\n",
      "\n",
      "Epoch: [26] Current learning rate (lr) = 1.0077695999999996e-05\n",
      "[epoch: 26, batch:   200] loss: 0.000\n",
      "[epoch: 26, batch:   400] loss: 0.000\n",
      "[epoch: 26, batch:   600] loss: 0.000\n",
      "[epoch: 26, batch:   800] loss: 0.000\n",
      "[epoch: 26, batch:  1000] loss: 0.000\n",
      "[epoch: 26, batch:  1200] loss: 0.000\n",
      "[epoch: 26, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.0133, Accuracy: 0.6374\n",
      "\n",
      "Epoch: [27] Current learning rate (lr) = 6.046617599999998e-06\n",
      "[epoch: 27, batch:   200] loss: 0.000\n",
      "[epoch: 27, batch:   400] loss: 0.000\n",
      "[epoch: 27, batch:   600] loss: 0.000\n",
      "[epoch: 27, batch:   800] loss: 0.000\n",
      "[epoch: 27, batch:  1000] loss: 0.000\n",
      "[epoch: 27, batch:  1200] loss: 0.000\n",
      "[epoch: 27, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.0483, Accuracy: 0.6371\n",
      "\n",
      "Epoch: [28] Current learning rate (lr) = 6.046617599999998e-06\n",
      "[epoch: 28, batch:   200] loss: 0.000\n",
      "[epoch: 28, batch:   400] loss: 0.000\n",
      "[epoch: 28, batch:   600] loss: 0.000\n",
      "[epoch: 28, batch:   800] loss: 0.000\n",
      "[epoch: 28, batch:  1000] loss: 0.000\n",
      "[epoch: 28, batch:  1200] loss: 0.000\n",
      "[epoch: 28, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.0871, Accuracy: 0.6369\n",
      "\n",
      "Epoch: [29] Current learning rate (lr) = 3.6279705599999985e-06\n",
      "[epoch: 29, batch:   200] loss: 0.000\n",
      "[epoch: 29, batch:   400] loss: 0.000\n",
      "[epoch: 29, batch:   600] loss: 0.000\n",
      "[epoch: 29, batch:   800] loss: 0.000\n",
      "[epoch: 29, batch:  1000] loss: 0.000\n",
      "[epoch: 29, batch:  1200] loss: 0.000\n",
      "[epoch: 29, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1109, Accuracy: 0.6364\n",
      "\n",
      "Epoch: [30] Current learning rate (lr) = 3.6279705599999985e-06\n",
      "[epoch: 30, batch:   200] loss: 0.000\n",
      "[epoch: 30, batch:   400] loss: 0.000\n",
      "[epoch: 30, batch:   600] loss: 0.000\n",
      "[epoch: 30, batch:   800] loss: 0.000\n",
      "[epoch: 30, batch:  1000] loss: 0.000\n",
      "[epoch: 30, batch:  1200] loss: 0.000\n",
      "[epoch: 30, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1364, Accuracy: 0.6362\n",
      "\n",
      "Epoch: [31] Current learning rate (lr) = 2.176782335999999e-06\n",
      "[epoch: 31, batch:   200] loss: 0.000\n",
      "[epoch: 31, batch:   400] loss: 0.000\n",
      "[epoch: 31, batch:   600] loss: 0.000\n",
      "[epoch: 31, batch:   800] loss: 0.000\n",
      "[epoch: 31, batch:  1000] loss: 0.000\n",
      "[epoch: 31, batch:  1200] loss: 0.000\n",
      "[epoch: 31, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1518, Accuracy: 0.6368\n",
      "\n",
      "Epoch: [32] Current learning rate (lr) = 2.176782335999999e-06\n",
      "[epoch: 32, batch:   200] loss: 0.000\n",
      "[epoch: 32, batch:   400] loss: 0.000\n",
      "[epoch: 32, batch:   600] loss: 0.000\n",
      "[epoch: 32, batch:   800] loss: 0.000\n",
      "[epoch: 32, batch:  1000] loss: 0.000\n",
      "[epoch: 32, batch:  1200] loss: 0.000\n",
      "[epoch: 32, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1679, Accuracy: 0.6368\n",
      "\n",
      "Epoch: [33] Current learning rate (lr) = 1.3060694015999993e-06\n",
      "[epoch: 33, batch:   200] loss: 0.000\n",
      "[epoch: 33, batch:   400] loss: 0.000\n",
      "[epoch: 33, batch:   600] loss: 0.000\n",
      "[epoch: 33, batch:   800] loss: 0.000\n",
      "[epoch: 33, batch:  1000] loss: 0.000\n",
      "[epoch: 33, batch:  1200] loss: 0.000\n",
      "[epoch: 33, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1775, Accuracy: 0.6369\n",
      "\n",
      "Epoch: [34] Current learning rate (lr) = 1.3060694015999993e-06\n",
      "[epoch: 34, batch:   200] loss: 0.000\n",
      "[epoch: 34, batch:   400] loss: 0.000\n",
      "[epoch: 34, batch:   600] loss: 0.000\n",
      "[epoch: 34, batch:   800] loss: 0.000\n",
      "[epoch: 34, batch:  1000] loss: 0.000\n",
      "[epoch: 34, batch:  1200] loss: 0.000\n",
      "[epoch: 34, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1874, Accuracy: 0.6365\n",
      "\n",
      "Epoch: [35] Current learning rate (lr) = 7.836416409599996e-07\n",
      "[epoch: 35, batch:   200] loss: 0.000\n",
      "[epoch: 35, batch:   400] loss: 0.000\n",
      "[epoch: 35, batch:   600] loss: 0.000\n",
      "[epoch: 35, batch:   800] loss: 0.000\n",
      "[epoch: 35, batch:  1000] loss: 0.000\n",
      "[epoch: 35, batch:  1200] loss: 0.000\n",
      "[epoch: 35, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1934, Accuracy: 0.6365\n",
      "\n",
      "Epoch: [36] Current learning rate (lr) = 7.836416409599996e-07\n",
      "[epoch: 36, batch:   200] loss: 0.000\n",
      "[epoch: 36, batch:   400] loss: 0.000\n",
      "[epoch: 36, batch:   600] loss: 0.000\n",
      "[epoch: 36, batch:   800] loss: 0.000\n",
      "[epoch: 36, batch:  1000] loss: 0.000\n",
      "[epoch: 36, batch:  1200] loss: 0.000\n",
      "[epoch: 36, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.1994, Accuracy: 0.6367\n",
      "\n",
      "Epoch: [37] Current learning rate (lr) = 4.7018498457599973e-07\n",
      "[epoch: 37, batch:   200] loss: 0.000\n",
      "[epoch: 37, batch:   400] loss: 0.000\n",
      "[epoch: 37, batch:   600] loss: 0.000\n",
      "[epoch: 37, batch:   800] loss: 0.000\n",
      "[epoch: 37, batch:  1000] loss: 0.000\n",
      "[epoch: 37, batch:  1200] loss: 0.000\n",
      "[epoch: 37, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.2031, Accuracy: 0.6366\n",
      "\n",
      "Epoch: [38] Current learning rate (lr) = 4.7018498457599973e-07\n",
      "[epoch: 38, batch:   200] loss: 0.000\n",
      "[epoch: 38, batch:   400] loss: 0.000\n",
      "[epoch: 38, batch:   600] loss: 0.000\n",
      "[epoch: 38, batch:   800] loss: 0.000\n",
      "[epoch: 38, batch:  1000] loss: 0.000\n",
      "[epoch: 38, batch:  1200] loss: 0.000\n",
      "[epoch: 38, batch:  1400] loss: 0.000\n",
      "[epoch: 39, batch:   200] loss: 0.000\n",
      "[epoch: 39, batch:   400] loss: 0.000\n",
      "[epoch: 39, batch:   600] loss: 0.000\n",
      "[epoch: 39, batch:   800] loss: 0.000\n",
      "[epoch: 39, batch:  1000] loss: 0.000\n",
      "[epoch: 39, batch:  1200] loss: 0.000\n",
      "[epoch: 39, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.2090, Accuracy: 0.6369\n",
      "\n",
      "Epoch: [40] Current learning rate (lr) = 2.821109907455998e-07\n",
      "[epoch: 40, batch:   200] loss: 0.000\n",
      "[epoch: 40, batch:   400] loss: 0.000\n",
      "[epoch: 40, batch:   600] loss: 0.000\n",
      "[epoch: 40, batch:   800] loss: 0.000\n",
      "[epoch: 40, batch:  1000] loss: 0.000\n",
      "[epoch: 40, batch:  1200] loss: 0.000\n",
      "[epoch: 40, batch:  1400] loss: 0.000\n",
      "\n",
      "Test set: Average loss: 3.2112, Accuracy: 0.6369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.hub.list('rwightman/gen-efficientnet-pytorch'))\n",
    "\n",
    "md = geffnet.create_model('efficientnet_b0',pretrained=False)\n",
    "\n",
    "md.classifier = torch.nn.Linear(1280,100,bias=False)\n",
    "\n",
    "# fine_tuning(md)\n",
    "\n",
    "main(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiiBu36AsHL1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "EfficientNet_Cifar100.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f474c3e87b4742a43aa43bd306cb31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16e844c36acb49968c4658b84c6791c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56c6650711ee4f95adcfd244d0a9d165": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6d46c0f9493f4d7289f38e6a8ed158ed": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8864d4f7405341a6be208d132866e659": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": " 36%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d46c0f9493f4d7289f38e6a8ed158ed",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56c6650711ee4f95adcfd244d0a9d165",
      "value": 1
     }
    },
    "c6e3d437239549b69f47e2e45b0f2442": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e92b75192404427180b16ac807e47d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6e3d437239549b69f47e2e45b0f2442",
      "placeholder": "​",
      "style": "IPY_MODEL_01f474c3e87b4742a43aa43bd306cb31",
      "value": " 61595648/169001437 [00:05&lt;00:06, 16510835.36it/s]"
     }
    },
    "fe4738ef0bfe46ee8677a43ccf013b7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8864d4f7405341a6be208d132866e659",
       "IPY_MODEL_e92b75192404427180b16ac807e47d8b"
      ],
      "layout": "IPY_MODEL_16e844c36acb49968c4658b84c6791c6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
