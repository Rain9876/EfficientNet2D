{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNet_Cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a5b094e92e248b1ab043a11548da9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1b0ec42c24948d2a79d1b0eda1c63d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ed426abf67d47a38971fc71589f80e7",
              "IPY_MODEL_422c3b187c4c47ac8ea04315b2cf8d79"
            ]
          }
        },
        "c1b0ec42c24948d2a79d1b0eda1c63d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ed426abf67d47a38971fc71589f80e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6965cb347e004e6d82f2bb4d0248ad14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98bf788d2cfd44108552d638478fad4a"
          }
        },
        "422c3b187c4c47ac8ea04315b2cf8d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26df67c10d4744528c462a9944913cca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:19&lt;00:00, 33500883.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb46b352cb4745759aee0b187b5961d7"
          }
        },
        "6965cb347e004e6d82f2bb4d0248ad14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98bf788d2cfd44108552d638478fad4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26df67c10d4744528c462a9944913cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb46b352cb4745759aee0b187b5961d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_WmNI_4fo_C",
        "colab_type": "code",
        "outputId": "1caa7d66-495e-4f36-a1ed-96b1dcf9c770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install geffnet\n",
        "!pip install timm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geffnet\n",
            "  Downloading https://files.pythonhosted.org/packages/04/a1/806cd0cbc1312f034e39cb4b3969430d8fe4f6cd0cb003dd223ba20e0d4d/geffnet-0.9.8-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from geffnet) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from geffnet) (0.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (1.18.2)\n",
            "Installing collected packages: geffnet\n",
            "Successfully installed geffnet-0.9.8\n",
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e9/dfee5381ae8e7862d8565cfc9ad7056dccbf2eefa214256da6b2fd878702/timm-0.1.18-py3-none-any.whl (158kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm) (0.5.0)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (1.18.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.1.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qqfVQEH8Y3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import geffnet\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "from timm.optim.radam import RAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhlvb5v88t1B",
        "colab_type": "code",
        "outputId": "f4533e7e-8ff6-47d4-b300-af625aaab8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(0)\n",
        "use_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
        "if use_GPU:\n",
        "    torch.cuda.manual_seed(0)\n",
        "print(\"Using GPU: {}\".format(use_GPU))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY_FM9iP8tof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ImageProcessing():\n",
        "    transform = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224),\n",
        "                                    transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "    train_dat = datasets.CIFAR10(root=sys.path[0] + \"/data/CIFAR10\", train=True, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dat, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    test_dat = datasets.CIFAR10(root=sys.path[0] + '/data/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dat, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpASQrai8tfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model, train_loader, optimizer, criterion, epoch):\n",
        "    training_loss = 0\n",
        "    model.train()\n",
        "    bi = 200\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        if batch_idx % bi == bi-1:  # print every 2000 mini-batches\n",
        "            print('[epoch: %d, batch: %5d] loss: %.4f' % (epoch + 1, batch_idx + 1, training_loss / bi))\n",
        "            training_loss = 0.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_jixg338tX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing(model, test_loader, criterion):\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, targets in test_loader:\n",
        "          \n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {:.4f}\\n\".format(test_loss / total,\n",
        "                                                                        correct / len(test_loader.dataset)))\n",
        "    return test_loss / total, correct / len(test_loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17YQBObMbVxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLhoy4wL8tP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tuning(model):\n",
        "\n",
        "    epochs = 20\n",
        "    lr = 0.0001\n",
        "\n",
        "    test_acc = []\n",
        "    test_loss = []\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    summary(model,(3,224,224),batch_size=10)\n",
        "\n",
        "    # optimizer = torch.optim.RMSprop(params_to_update, lr = 0.0001, weight_decay =1e-5)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.99, weight_decay=1e-5)\n",
        "    # optimizer = torch.optim.SGD(params_to_update, lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
        "\n",
        "    # train_loader, test_loader = ImageProcessingForFolder(\"****************\", 32, resolution, img_stats)\n",
        "    \n",
        "    # optimizer = torch.optim.Adam(model.parameters(),lr=lr,  eps=1e-5, weight_decay=1e-5)\n",
        "    \n",
        "    # reduce_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.6)\n",
        "    reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1)\n",
        "\n",
        "    train_loader, test_loader = ImageProcessing()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print_learning_rate(optimizer,epoch+1)\n",
        "\n",
        "        training(model, train_loader, optimizer, criterion, epoch)\n",
        "\n",
        "        loss, acc = testing(model, test_loader, criterion)\n",
        "\n",
        "        test_loss.append(loss)\n",
        "\n",
        "        test_acc.append(acc)\n",
        "\n",
        "        # adjust_learning_rate(0.0001, optimizer,epoch+1)\n",
        "\n",
        "        reduce_lr.step(loss)\n",
        "\n",
        "\n",
        "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
        "    ax1.plot(list(range(1,epochs+1)), test_acc)\n",
        "    ax1.set(xlabel='epochs', ylabel='test accuracy')\n",
        "    ax2.plot(list(range(1,epochs+1)), test_loss)\n",
        "    ax2.set(xlabel='epochs', ylabel='test loss')\n",
        "    fig.tight_layout(pad=4.0)\n",
        "    ax2.set_xticks(np.arange(1, epochs+1, step=1))\n",
        "    ax1.set_xticks(np.arange(1, epochs+1, step=1))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiBCFMeKkM8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adjust_learning_rate(lr, optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 0.97 every 2.4 epochs\"\"\"\n",
        "    lr = lr * (0.9 ** (epoch // 2))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def print_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(\"Epoch: [{}] Current learning rate (lr) = {}\".format(\n",
        "                                                    epoch, param_group['lr']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEBDPl1uElTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(md):\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    # classes = (\n",
        "    # 'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
        "    # 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
        "    # 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
        "    # 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
        "    # 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
        "    # 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    # 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    # 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    # 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    # 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "    # 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "    # 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "    # 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "    # 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
        "    # 'worm')\n",
        "\n",
        "    train_loader, test_loader = ImageProcessing()\n",
        "    \n",
        "    # print(len(classes))\n",
        "    # imageshow(train_loader, classes)\n",
        "\n",
        "    momentum = 0.9\n",
        "    epochs = 20\n",
        "    decay = 1e-5\n",
        "    lr = 0.001\n",
        "\n",
        "    model = md.to(device)\n",
        "\n",
        "    summary(model,(3,224,224),batch_size=10)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    optimizer = RAdam(model.parameters(),lr=lr,  eps=1e-5, weight_decay=decay)\n",
        "\n",
        "    # reduce_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "    reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print_learning_rate(optimizer,epoch+1)\n",
        "        training(model, train_loader, optimizer, criterion, epoch)\n",
        "        loss,_ = testing(model, test_loader, criterion)\n",
        "        # adjust_learning_rate(lr, optimizer,epoch+1)\n",
        "        reduce_lr.step(loss)\n",
        "\n",
        "    PATH = './cifar_net.pth'\n",
        "    torch.save(model.state_dict(), PATH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OdtW7qA8OzU",
        "colab_type": "code",
        "outputId": "2bb19ee4-1c14-4c8b-f3ec-7fa579d1bfc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3a5b094e92e248b1ab043a11548da9b0",
            "c1b0ec42c24948d2a79d1b0eda1c63d4",
            "0ed426abf67d47a38971fc71589f80e7",
            "422c3b187c4c47ac8ea04315b2cf8d79",
            "6965cb347e004e6d82f2bb4d0248ad14",
            "98bf788d2cfd44108552d638478fad4a",
            "26df67c10d4744528c462a9944913cca",
            "bb46b352cb4745759aee0b187b5961d7"
          ]
        }
      },
      "source": [
        "print(torch.hub.list('rwightman/gen-efficientnet-pytorch'))\n",
        "\n",
        "md = geffnet.create_model('efficientnet_b0',pretrained=False)\n",
        "\n",
        "md.classifier = torch.nn.Linear(1280,10,bias=True)\n",
        "\n",
        "main(md)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/gen-efficientnet-pytorch/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_es', 'fbnetc_100', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_a1', 'mnasnet_b1', 'mobilenetv3_large_100', 'mobilenetv3_rw', 'spnasnet_100', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100']\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/CIFAR10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a5b094e92e248b1ab043a11548da9b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /data/CIFAR10/cifar-10-python.tar.gz to /data/CIFAR10\n",
            "Files already downloaded and verified\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [10, 32, 112, 112]             864\n",
            "       BatchNorm2d-2         [10, 32, 112, 112]              64\n",
            "          SwishJit-3         [10, 32, 112, 112]               0\n",
            "            Conv2d-4         [10, 32, 112, 112]             288\n",
            "       BatchNorm2d-5         [10, 32, 112, 112]              64\n",
            "          SwishJit-6         [10, 32, 112, 112]               0\n",
            " AdaptiveAvgPool2d-7             [10, 32, 1, 1]               0\n",
            "            Conv2d-8              [10, 8, 1, 1]             264\n",
            "          SwishJit-9              [10, 8, 1, 1]               0\n",
            "           Conv2d-10             [10, 32, 1, 1]             288\n",
            "    SqueezeExcite-11         [10, 32, 112, 112]               0\n",
            "           Conv2d-12         [10, 16, 112, 112]             512\n",
            "      BatchNorm2d-13         [10, 16, 112, 112]              32\n",
            "         Identity-14         [10, 16, 112, 112]               0\n",
            "DepthwiseSeparableConv-15         [10, 16, 112, 112]               0\n",
            "           Conv2d-16         [10, 96, 112, 112]           1,536\n",
            "      BatchNorm2d-17         [10, 96, 112, 112]             192\n",
            "         SwishJit-18         [10, 96, 112, 112]               0\n",
            "           Conv2d-19           [10, 96, 56, 56]             864\n",
            "      BatchNorm2d-20           [10, 96, 56, 56]             192\n",
            "         SwishJit-21           [10, 96, 56, 56]               0\n",
            "AdaptiveAvgPool2d-22             [10, 96, 1, 1]               0\n",
            "           Conv2d-23              [10, 4, 1, 1]             388\n",
            "         SwishJit-24              [10, 4, 1, 1]               0\n",
            "           Conv2d-25             [10, 96, 1, 1]             480\n",
            "    SqueezeExcite-26           [10, 96, 56, 56]               0\n",
            "           Conv2d-27           [10, 24, 56, 56]           2,304\n",
            "      BatchNorm2d-28           [10, 24, 56, 56]              48\n",
            " InvertedResidual-29           [10, 24, 56, 56]               0\n",
            "           Conv2d-30          [10, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-31          [10, 144, 56, 56]             288\n",
            "         SwishJit-32          [10, 144, 56, 56]               0\n",
            "           Conv2d-33          [10, 144, 56, 56]           1,296\n",
            "      BatchNorm2d-34          [10, 144, 56, 56]             288\n",
            "         SwishJit-35          [10, 144, 56, 56]               0\n",
            "AdaptiveAvgPool2d-36            [10, 144, 1, 1]               0\n",
            "           Conv2d-37              [10, 6, 1, 1]             870\n",
            "         SwishJit-38              [10, 6, 1, 1]               0\n",
            "           Conv2d-39            [10, 144, 1, 1]           1,008\n",
            "    SqueezeExcite-40          [10, 144, 56, 56]               0\n",
            "           Conv2d-41           [10, 24, 56, 56]           3,456\n",
            "      BatchNorm2d-42           [10, 24, 56, 56]              48\n",
            " InvertedResidual-43           [10, 24, 56, 56]               0\n",
            "           Conv2d-44          [10, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-45          [10, 144, 56, 56]             288\n",
            "         SwishJit-46          [10, 144, 56, 56]               0\n",
            "           Conv2d-47          [10, 144, 28, 28]           3,600\n",
            "      BatchNorm2d-48          [10, 144, 28, 28]             288\n",
            "         SwishJit-49          [10, 144, 28, 28]               0\n",
            "AdaptiveAvgPool2d-50            [10, 144, 1, 1]               0\n",
            "           Conv2d-51              [10, 6, 1, 1]             870\n",
            "         SwishJit-52              [10, 6, 1, 1]               0\n",
            "           Conv2d-53            [10, 144, 1, 1]           1,008\n",
            "    SqueezeExcite-54          [10, 144, 28, 28]               0\n",
            "           Conv2d-55           [10, 40, 28, 28]           5,760\n",
            "      BatchNorm2d-56           [10, 40, 28, 28]              80\n",
            " InvertedResidual-57           [10, 40, 28, 28]               0\n",
            "           Conv2d-58          [10, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-59          [10, 240, 28, 28]             480\n",
            "         SwishJit-60          [10, 240, 28, 28]               0\n",
            "           Conv2d-61          [10, 240, 28, 28]           6,000\n",
            "      BatchNorm2d-62          [10, 240, 28, 28]             480\n",
            "         SwishJit-63          [10, 240, 28, 28]               0\n",
            "AdaptiveAvgPool2d-64            [10, 240, 1, 1]               0\n",
            "           Conv2d-65             [10, 10, 1, 1]           2,410\n",
            "         SwishJit-66             [10, 10, 1, 1]               0\n",
            "           Conv2d-67            [10, 240, 1, 1]           2,640\n",
            "    SqueezeExcite-68          [10, 240, 28, 28]               0\n",
            "           Conv2d-69           [10, 40, 28, 28]           9,600\n",
            "      BatchNorm2d-70           [10, 40, 28, 28]              80\n",
            " InvertedResidual-71           [10, 40, 28, 28]               0\n",
            "           Conv2d-72          [10, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-73          [10, 240, 28, 28]             480\n",
            "         SwishJit-74          [10, 240, 28, 28]               0\n",
            "           Conv2d-75          [10, 240, 14, 14]           2,160\n",
            "      BatchNorm2d-76          [10, 240, 14, 14]             480\n",
            "         SwishJit-77          [10, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-78            [10, 240, 1, 1]               0\n",
            "           Conv2d-79             [10, 10, 1, 1]           2,410\n",
            "         SwishJit-80             [10, 10, 1, 1]               0\n",
            "           Conv2d-81            [10, 240, 1, 1]           2,640\n",
            "    SqueezeExcite-82          [10, 240, 14, 14]               0\n",
            "           Conv2d-83           [10, 80, 14, 14]          19,200\n",
            "      BatchNorm2d-84           [10, 80, 14, 14]             160\n",
            " InvertedResidual-85           [10, 80, 14, 14]               0\n",
            "           Conv2d-86          [10, 480, 14, 14]          38,400\n",
            "      BatchNorm2d-87          [10, 480, 14, 14]             960\n",
            "         SwishJit-88          [10, 480, 14, 14]               0\n",
            "           Conv2d-89          [10, 480, 14, 14]           4,320\n",
            "      BatchNorm2d-90          [10, 480, 14, 14]             960\n",
            "         SwishJit-91          [10, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-92            [10, 480, 1, 1]               0\n",
            "           Conv2d-93             [10, 20, 1, 1]           9,620\n",
            "         SwishJit-94             [10, 20, 1, 1]               0\n",
            "           Conv2d-95            [10, 480, 1, 1]          10,080\n",
            "    SqueezeExcite-96          [10, 480, 14, 14]               0\n",
            "           Conv2d-97           [10, 80, 14, 14]          38,400\n",
            "      BatchNorm2d-98           [10, 80, 14, 14]             160\n",
            " InvertedResidual-99           [10, 80, 14, 14]               0\n",
            "          Conv2d-100          [10, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-101          [10, 480, 14, 14]             960\n",
            "        SwishJit-102          [10, 480, 14, 14]               0\n",
            "          Conv2d-103          [10, 480, 14, 14]           4,320\n",
            "     BatchNorm2d-104          [10, 480, 14, 14]             960\n",
            "        SwishJit-105          [10, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-106            [10, 480, 1, 1]               0\n",
            "          Conv2d-107             [10, 20, 1, 1]           9,620\n",
            "        SwishJit-108             [10, 20, 1, 1]               0\n",
            "          Conv2d-109            [10, 480, 1, 1]          10,080\n",
            "   SqueezeExcite-110          [10, 480, 14, 14]               0\n",
            "          Conv2d-111           [10, 80, 14, 14]          38,400\n",
            "     BatchNorm2d-112           [10, 80, 14, 14]             160\n",
            "InvertedResidual-113           [10, 80, 14, 14]               0\n",
            "          Conv2d-114          [10, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-115          [10, 480, 14, 14]             960\n",
            "        SwishJit-116          [10, 480, 14, 14]               0\n",
            "          Conv2d-117          [10, 480, 14, 14]          12,000\n",
            "     BatchNorm2d-118          [10, 480, 14, 14]             960\n",
            "        SwishJit-119          [10, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-120            [10, 480, 1, 1]               0\n",
            "          Conv2d-121             [10, 20, 1, 1]           9,620\n",
            "        SwishJit-122             [10, 20, 1, 1]               0\n",
            "          Conv2d-123            [10, 480, 1, 1]          10,080\n",
            "   SqueezeExcite-124          [10, 480, 14, 14]               0\n",
            "          Conv2d-125          [10, 112, 14, 14]          53,760\n",
            "     BatchNorm2d-126          [10, 112, 14, 14]             224\n",
            "InvertedResidual-127          [10, 112, 14, 14]               0\n",
            "          Conv2d-128          [10, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-129          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-130          [10, 672, 14, 14]               0\n",
            "          Conv2d-131          [10, 672, 14, 14]          16,800\n",
            "     BatchNorm2d-132          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-133          [10, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-134            [10, 672, 1, 1]               0\n",
            "          Conv2d-135             [10, 28, 1, 1]          18,844\n",
            "        SwishJit-136             [10, 28, 1, 1]               0\n",
            "          Conv2d-137            [10, 672, 1, 1]          19,488\n",
            "   SqueezeExcite-138          [10, 672, 14, 14]               0\n",
            "          Conv2d-139          [10, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-140          [10, 112, 14, 14]             224\n",
            "InvertedResidual-141          [10, 112, 14, 14]               0\n",
            "          Conv2d-142          [10, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-143          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-144          [10, 672, 14, 14]               0\n",
            "          Conv2d-145          [10, 672, 14, 14]          16,800\n",
            "     BatchNorm2d-146          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-147          [10, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-148            [10, 672, 1, 1]               0\n",
            "          Conv2d-149             [10, 28, 1, 1]          18,844\n",
            "        SwishJit-150             [10, 28, 1, 1]               0\n",
            "          Conv2d-151            [10, 672, 1, 1]          19,488\n",
            "   SqueezeExcite-152          [10, 672, 14, 14]               0\n",
            "          Conv2d-153          [10, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-154          [10, 112, 14, 14]             224\n",
            "InvertedResidual-155          [10, 112, 14, 14]               0\n",
            "          Conv2d-156          [10, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-157          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-158          [10, 672, 14, 14]               0\n",
            "          Conv2d-159            [10, 672, 7, 7]          16,800\n",
            "     BatchNorm2d-160            [10, 672, 7, 7]           1,344\n",
            "        SwishJit-161            [10, 672, 7, 7]               0\n",
            "AdaptiveAvgPool2d-162            [10, 672, 1, 1]               0\n",
            "          Conv2d-163             [10, 28, 1, 1]          18,844\n",
            "        SwishJit-164             [10, 28, 1, 1]               0\n",
            "          Conv2d-165            [10, 672, 1, 1]          19,488\n",
            "   SqueezeExcite-166            [10, 672, 7, 7]               0\n",
            "          Conv2d-167            [10, 192, 7, 7]         129,024\n",
            "     BatchNorm2d-168            [10, 192, 7, 7]             384\n",
            "InvertedResidual-169            [10, 192, 7, 7]               0\n",
            "          Conv2d-170           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-171           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-172           [10, 1152, 7, 7]               0\n",
            "          Conv2d-173           [10, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-174           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-175           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-176           [10, 1152, 1, 1]               0\n",
            "          Conv2d-177             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-178             [10, 48, 1, 1]               0\n",
            "          Conv2d-179           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-180           [10, 1152, 7, 7]               0\n",
            "          Conv2d-181            [10, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-182            [10, 192, 7, 7]             384\n",
            "InvertedResidual-183            [10, 192, 7, 7]               0\n",
            "          Conv2d-184           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-185           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-186           [10, 1152, 7, 7]               0\n",
            "          Conv2d-187           [10, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-188           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-189           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-190           [10, 1152, 1, 1]               0\n",
            "          Conv2d-191             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-192             [10, 48, 1, 1]               0\n",
            "          Conv2d-193           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-194           [10, 1152, 7, 7]               0\n",
            "          Conv2d-195            [10, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-196            [10, 192, 7, 7]             384\n",
            "InvertedResidual-197            [10, 192, 7, 7]               0\n",
            "          Conv2d-198           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-199           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-200           [10, 1152, 7, 7]               0\n",
            "          Conv2d-201           [10, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-202           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-203           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-204           [10, 1152, 1, 1]               0\n",
            "          Conv2d-205             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-206             [10, 48, 1, 1]               0\n",
            "          Conv2d-207           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-208           [10, 1152, 7, 7]               0\n",
            "          Conv2d-209            [10, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-210            [10, 192, 7, 7]             384\n",
            "InvertedResidual-211            [10, 192, 7, 7]               0\n",
            "          Conv2d-212           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-213           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-214           [10, 1152, 7, 7]               0\n",
            "          Conv2d-215           [10, 1152, 7, 7]          10,368\n",
            "     BatchNorm2d-216           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-217           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-218           [10, 1152, 1, 1]               0\n",
            "          Conv2d-219             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-220             [10, 48, 1, 1]               0\n",
            "          Conv2d-221           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-222           [10, 1152, 7, 7]               0\n",
            "          Conv2d-223            [10, 320, 7, 7]         368,640\n",
            "     BatchNorm2d-224            [10, 320, 7, 7]             640\n",
            "InvertedResidual-225            [10, 320, 7, 7]               0\n",
            "          Conv2d-226           [10, 1280, 7, 7]         409,600\n",
            "     BatchNorm2d-227           [10, 1280, 7, 7]           2,560\n",
            "        SwishJit-228           [10, 1280, 7, 7]               0\n",
            "AdaptiveAvgPool2d-229           [10, 1280, 1, 1]               0\n",
            "          Linear-230                   [10, 10]          12,810\n",
            "================================================================\n",
            "Total params: 4,020,358\n",
            "Trainable params: 4,020,358\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 5.74\n",
            "Forward/backward pass size (MB): 1734.95\n",
            "Params size (MB): 15.34\n",
            "Estimated Total Size (MB): 1756.03\n",
            "----------------------------------------------------------------\n",
            "Epoch: [1] Current learning rate (lr) = 0.001\n",
            "[epoch: 1, batch:   200] loss: 2.0438\n",
            "[epoch: 1, batch:   400] loss: 1.6979\n",
            "[epoch: 1, batch:   600] loss: 1.5401\n",
            "[epoch: 1, batch:   800] loss: 1.3825\n",
            "[epoch: 1, batch:  1000] loss: 1.2570\n",
            "[epoch: 1, batch:  1200] loss: 1.1947\n",
            "[epoch: 1, batch:  1400] loss: 1.1002\n",
            "\n",
            "Test set: Average loss: 1.0227, Accuracy: 0.6400\n",
            "\n",
            "Epoch: [2] Current learning rate (lr) = 0.001\n",
            "[epoch: 2, batch:   200] loss: 0.9613\n",
            "[epoch: 2, batch:   400] loss: 0.9377\n",
            "[epoch: 2, batch:   600] loss: 0.8782\n",
            "[epoch: 2, batch:   800] loss: 0.8170\n",
            "[epoch: 2, batch:  1000] loss: 0.7796\n",
            "[epoch: 2, batch:  1200] loss: 0.7749\n",
            "[epoch: 2, batch:  1400] loss: 0.7103\n",
            "\n",
            "Test set: Average loss: 0.7153, Accuracy: 0.7485\n",
            "\n",
            "Epoch: [3] Current learning rate (lr) = 0.001\n",
            "[epoch: 3, batch:   200] loss: 0.6589\n",
            "[epoch: 3, batch:   400] loss: 0.6466\n",
            "[epoch: 3, batch:   600] loss: 0.6122\n",
            "[epoch: 3, batch:   800] loss: 0.5843\n",
            "[epoch: 3, batch:  1000] loss: 0.5605\n",
            "[epoch: 3, batch:  1200] loss: 0.5615\n",
            "[epoch: 3, batch:  1400] loss: 0.5118\n",
            "\n",
            "Test set: Average loss: 0.6810, Accuracy: 0.7675\n",
            "\n",
            "Epoch: [4] Current learning rate (lr) = 0.001\n",
            "[epoch: 4, batch:   200] loss: 0.4897\n",
            "[epoch: 4, batch:   400] loss: 0.4674\n",
            "[epoch: 4, batch:   600] loss: 0.4560\n",
            "[epoch: 4, batch:   800] loss: 0.4374\n",
            "[epoch: 4, batch:  1000] loss: 0.4084\n",
            "[epoch: 4, batch:  1200] loss: 0.4058\n",
            "[epoch: 4, batch:  1400] loss: 0.3727\n",
            "\n",
            "Test set: Average loss: 0.6439, Accuracy: 0.7938\n",
            "\n",
            "Epoch: [5] Current learning rate (lr) = 0.001\n",
            "[epoch: 5, batch:   200] loss: 0.3486\n",
            "[epoch: 5, batch:   400] loss: 0.3486\n",
            "[epoch: 5, batch:   600] loss: 0.3475\n",
            "[epoch: 5, batch:   800] loss: 0.3450\n",
            "[epoch: 5, batch:  1000] loss: 0.2979\n",
            "[epoch: 5, batch:  1200] loss: 0.2983\n",
            "[epoch: 5, batch:  1400] loss: 0.2937\n",
            "\n",
            "Test set: Average loss: 0.6827, Accuracy: 0.7949\n",
            "\n",
            "Epoch: [6] Current learning rate (lr) = 0.001\n",
            "[epoch: 6, batch:   200] loss: 0.2574\n",
            "[epoch: 6, batch:   400] loss: 0.2610\n",
            "[epoch: 6, batch:   600] loss: 0.2867\n",
            "[epoch: 6, batch:   800] loss: 0.2518\n",
            "[epoch: 6, batch:  1000] loss: 0.2274\n",
            "[epoch: 6, batch:  1200] loss: 0.2412\n",
            "[epoch: 6, batch:  1400] loss: 0.2217\n",
            "\n",
            "Test set: Average loss: 0.7004, Accuracy: 0.8053\n",
            "\n",
            "Epoch: [7] Current learning rate (lr) = 0.001\n",
            "[epoch: 7, batch:   200] loss: 0.2279\n",
            "[epoch: 7, batch:   400] loss: 0.2037\n",
            "[epoch: 7, batch:   600] loss: 0.2052\n",
            "[epoch: 7, batch:   800] loss: 0.2063\n",
            "[epoch: 7, batch:  1000] loss: 0.1592\n",
            "[epoch: 7, batch:  1200] loss: 0.1780\n",
            "[epoch: 7, batch:  1400] loss: 0.1768\n",
            "\n",
            "Test set: Average loss: 0.7046, Accuracy: 0.8099\n",
            "\n",
            "Epoch: [8] Current learning rate (lr) = 0.0005\n",
            "[epoch: 8, batch:   200] loss: 0.1309\n",
            "[epoch: 8, batch:   400] loss: 0.1136\n",
            "[epoch: 8, batch:   600] loss: 0.0987\n",
            "[epoch: 8, batch:   800] loss: 0.0888\n",
            "[epoch: 8, batch:  1000] loss: 0.0706\n",
            "[epoch: 8, batch:  1200] loss: 0.0676\n",
            "[epoch: 8, batch:  1400] loss: 0.0549\n",
            "\n",
            "Test set: Average loss: 0.6846, Accuracy: 0.8408\n",
            "\n",
            "Epoch: [9] Current learning rate (lr) = 0.0005\n",
            "[epoch: 9, batch:   200] loss: 0.0433\n",
            "[epoch: 9, batch:   400] loss: 0.0378\n",
            "[epoch: 9, batch:   600] loss: 0.0366\n",
            "[epoch: 9, batch:   800] loss: 0.0408\n",
            "[epoch: 9, batch:  1000] loss: 0.0371\n",
            "[epoch: 9, batch:  1200] loss: 0.0351\n",
            "[epoch: 9, batch:  1400] loss: 0.0289\n",
            "\n",
            "Test set: Average loss: 0.7929, Accuracy: 0.8412\n",
            "\n",
            "Epoch: [10] Current learning rate (lr) = 0.0005\n",
            "[epoch: 10, batch:   200] loss: 0.0408\n",
            "[epoch: 10, batch:   400] loss: 0.0409\n",
            "[epoch: 10, batch:   600] loss: 0.0494\n",
            "[epoch: 10, batch:   800] loss: 0.0496\n",
            "[epoch: 10, batch:  1000] loss: 0.0421\n",
            "[epoch: 10, batch:  1200] loss: 0.0417\n",
            "[epoch: 10, batch:  1400] loss: 0.0443\n",
            "\n",
            "Test set: Average loss: 0.8379, Accuracy: 0.8383\n",
            "\n",
            "Epoch: [11] Current learning rate (lr) = 0.00025\n",
            "[epoch: 11, batch:   200] loss: 0.0347\n",
            "[epoch: 11, batch:   400] loss: 0.0225\n",
            "[epoch: 11, batch:   600] loss: 0.0190\n",
            "[epoch: 11, batch:   800] loss: 0.0159\n",
            "[epoch: 11, batch:  1000] loss: 0.0151\n",
            "[epoch: 11, batch:  1200] loss: 0.0103\n",
            "[epoch: 11, batch:  1400] loss: 0.0073\n",
            "\n",
            "Test set: Average loss: 0.7897, Accuracy: 0.8561\n",
            "\n",
            "Epoch: [12] Current learning rate (lr) = 0.00025\n",
            "[epoch: 12, batch:   200] loss: 0.0078\n",
            "[epoch: 12, batch:   400] loss: 0.0060\n",
            "[epoch: 12, batch:   600] loss: 0.0072\n",
            "[epoch: 12, batch:   800] loss: 0.0049\n",
            "[epoch: 12, batch:  1000] loss: 0.0034\n",
            "[epoch: 12, batch:  1200] loss: 0.0023\n",
            "[epoch: 12, batch:  1400] loss: 0.0025\n",
            "\n",
            "Test set: Average loss: 0.8592, Accuracy: 0.8566\n",
            "\n",
            "Epoch: [13] Current learning rate (lr) = 0.00025\n",
            "[epoch: 13, batch:   200] loss: 0.0017\n",
            "[epoch: 13, batch:   400] loss: 0.0016\n",
            "[epoch: 13, batch:   600] loss: 0.0020\n",
            "[epoch: 13, batch:   800] loss: 0.0050\n",
            "[epoch: 13, batch:  1000] loss: 0.0026\n",
            "[epoch: 13, batch:  1200] loss: 0.0076\n",
            "[epoch: 13, batch:  1400] loss: 0.0093\n",
            "\n",
            "Test set: Average loss: 1.0032, Accuracy: 0.8423\n",
            "\n",
            "Epoch: [14] Current learning rate (lr) = 0.000125\n",
            "[epoch: 14, batch:   200] loss: 0.0125\n",
            "[epoch: 14, batch:   400] loss: 0.0086\n",
            "[epoch: 14, batch:   600] loss: 0.0069\n",
            "[epoch: 14, batch:   800] loss: 0.0035\n",
            "[epoch: 14, batch:  1000] loss: 0.0026\n",
            "[epoch: 14, batch:  1200] loss: 0.0026\n",
            "[epoch: 14, batch:  1400] loss: 0.0014\n",
            "\n",
            "Test set: Average loss: 0.8975, Accuracy: 0.8553\n",
            "\n",
            "Epoch: [15] Current learning rate (lr) = 0.000125\n",
            "[epoch: 15, batch:   200] loss: 0.0010\n",
            "[epoch: 15, batch:   400] loss: 0.0008\n",
            "[epoch: 15, batch:   600] loss: 0.0006\n",
            "[epoch: 15, batch:   800] loss: 0.0004\n",
            "[epoch: 15, batch:  1000] loss: 0.0003\n",
            "[epoch: 15, batch:  1200] loss: 0.0002\n",
            "[epoch: 15, batch:  1400] loss: 0.0002\n",
            "\n",
            "Test set: Average loss: 0.9068, Accuracy: 0.8577\n",
            "\n",
            "Epoch: [16] Current learning rate (lr) = 0.000125\n",
            "[epoch: 16, batch:   200] loss: 0.0001\n",
            "[epoch: 16, batch:   400] loss: 0.0002\n",
            "[epoch: 16, batch:   600] loss: 0.0001\n",
            "[epoch: 16, batch:   800] loss: 0.0001\n",
            "[epoch: 16, batch:  1000] loss: 0.0001\n",
            "[epoch: 16, batch:  1200] loss: 0.0001\n",
            "[epoch: 16, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.9268, Accuracy: 0.8584\n",
            "\n",
            "Epoch: [17] Current learning rate (lr) = 6.25e-05\n",
            "[epoch: 17, batch:   200] loss: 0.0001\n",
            "[epoch: 17, batch:   400] loss: 0.0001\n",
            "[epoch: 17, batch:   600] loss: 0.0001\n",
            "[epoch: 17, batch:   800] loss: 0.0001\n",
            "[epoch: 17, batch:  1000] loss: 0.0001\n",
            "[epoch: 17, batch:  1200] loss: 0.0001\n",
            "[epoch: 17, batch:  1400] loss: 0.0000\n",
            "\n",
            "Test set: Average loss: 0.9382, Accuracy: 0.8589\n",
            "\n",
            "Epoch: [18] Current learning rate (lr) = 6.25e-05\n",
            "[epoch: 18, batch:   200] loss: 0.0001\n",
            "[epoch: 18, batch:   400] loss: 0.0001\n",
            "[epoch: 18, batch:   600] loss: 0.0001\n",
            "[epoch: 18, batch:   800] loss: 0.0001\n",
            "[epoch: 18, batch:  1000] loss: 0.0000\n",
            "[epoch: 18, batch:  1200] loss: 0.0000\n",
            "[epoch: 18, batch:  1400] loss: 0.0000\n",
            "\n",
            "Test set: Average loss: 0.9533, Accuracy: 0.8593\n",
            "\n",
            "Epoch: [19] Current learning rate (lr) = 6.25e-05\n",
            "[epoch: 19, batch:   200] loss: 0.0000\n",
            "[epoch: 19, batch:   400] loss: 0.0000\n",
            "[epoch: 19, batch:   600] loss: 0.0000\n",
            "[epoch: 19, batch:   800] loss: 0.0000\n",
            "[epoch: 19, batch:  1000] loss: 0.0000\n",
            "[epoch: 19, batch:  1200] loss: 0.0000\n",
            "[epoch: 19, batch:  1400] loss: 0.0000\n",
            "\n",
            "Test set: Average loss: 0.9698, Accuracy: 0.8585\n",
            "\n",
            "Epoch: [20] Current learning rate (lr) = 3.125e-05\n",
            "[epoch: 20, batch:   200] loss: 0.0000\n",
            "[epoch: 20, batch:   400] loss: 0.0000\n",
            "[epoch: 20, batch:   600] loss: 0.0000\n",
            "[epoch: 20, batch:   800] loss: 0.0000\n",
            "[epoch: 20, batch:  1000] loss: 0.0000\n",
            "[epoch: 20, batch:  1200] loss: 0.0000\n",
            "[epoch: 20, batch:  1400] loss: 0.0000\n",
            "\n",
            "Test set: Average loss: 0.9778, Accuracy: 0.8591\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODoDGptnnEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}