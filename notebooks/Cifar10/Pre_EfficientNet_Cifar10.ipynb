{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_EfficientNet_Cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_WmNI_4fo_C",
        "colab_type": "code",
        "outputId": "a9392050-2dc5-4fe8-b691-fbe9cf8cee97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install geffnet\n",
        "!pip install timm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geffnet in /usr/local/lib/python3.6/dist-packages (0.9.8)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from geffnet) (0.5.0)\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from geffnet) (1.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->geffnet) (1.12.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.6/dist-packages (0.1.18)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm) (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qqfVQEH8Y3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import geffnet\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "from timm.optim.radam import RAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhlvb5v88t1B",
        "colab_type": "code",
        "outputId": "1ebd8b73-d59a-4608-f0de-c254b4a568f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(0)\n",
        "use_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
        "if use_GPU:\n",
        "    torch.cuda.manual_seed(0)\n",
        "print(\"Using GPU: {}\".format(use_GPU))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY_FM9iP8tof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ImageProcessing():\n",
        "    transform = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224),\n",
        "                                    transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "    train_dat = datasets.CIFAR10(root=sys.path[0] + \"/data/CIFAR10\", train=True, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dat, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    test_dat = datasets.CIFAR10(root=sys.path[0] + '/data/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dat, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpASQrai8tfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model, train_loader, optimizer, criterion, epoch):\n",
        "    training_loss = 0\n",
        "    model.train()\n",
        "    bi = 200\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        if batch_idx % bi == bi-1:  # print every 2000 mini-batches\n",
        "            print('[epoch: %d, batch: %5d] loss: %.4f' % (epoch + 1, batch_idx + 1, training_loss / bi))\n",
        "            training_loss = 0.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_jixg338tX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing(model, test_loader, criterion):\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, targets in test_loader:\n",
        "          \n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {:.4f}\\n\".format(test_loss / total,\n",
        "                                                                        correct / len(test_loader.dataset)))\n",
        "    return test_loss / total, correct / len(test_loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9hWMxhMuQui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predicition(md, loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for inputs, targets in loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            y_pred.append(predicted)\n",
        "            \n",
        "            y_true.append(targets)\n",
        "\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {:.4f}\\n\".format(test_loss / total,\n",
        "                                                                        correct / len(test_loader.dataset)))\n",
        "    return correct / len(test_loader.dataset), y_pred, y_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17YQBObMbVxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLhoy4wL8tP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tuning(model):\n",
        "\n",
        "    epochs = 20\n",
        "    lr = 0.0001\n",
        "\n",
        "    test_acc = []\n",
        "    test_loss = []\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    summary(model,(3,224,224),batch_size=10)\n",
        "\n",
        "    # optimizer = torch.optim.RMSprop(params_to_update, lr = 0.0001, weight_decay =1e-5)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.99, weight_decay=1e-5)\n",
        "    # optimizer = torch.optim.SGD(params_to_update, lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
        "\n",
        "    # train_loader, test_loader = ImageProcessingForFolder(\"****************\", 32, resolution, img_stats)\n",
        "    \n",
        "    # optimizer = torch.optim.Adam(model.parameters(),lr=lr,  eps=1e-5, weight_decay=1e-5)\n",
        "    \n",
        "    # reduce_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.6)\n",
        "    reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1)\n",
        "\n",
        "    train_loader, test_loader = ImageProcessing()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print_learning_rate(optimizer,epoch+1)\n",
        "\n",
        "        training(model, train_loader, optimizer, criterion, epoch)\n",
        "\n",
        "        loss, acc = testing(model, test_loader, criterion)\n",
        "\n",
        "        test_loss.append(loss)\n",
        "\n",
        "        test_acc.append(acc)\n",
        "\n",
        "        # adjust_learning_rate(0.0001, optimizer,epoch+1)\n",
        "\n",
        "        reduce_lr.step(loss)\n",
        "\n",
        "\n",
        "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
        "    ax1.plot(list(range(1,epochs+1)), test_acc)\n",
        "    ax1.set(xlabel='epochs', ylabel='test accuracy')\n",
        "    ax2.plot(list(range(1,epochs+1)), test_loss)\n",
        "    ax2.set(xlabel='epochs', ylabel='test loss')\n",
        "    fig.tight_layout(pad=4.0)\n",
        "    ax2.set_xticks(np.arange(1, epochs+1, step=1))\n",
        "    ax1.set_xticks(np.arange(1, epochs+1, step=1))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiBCFMeKkM8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adjust_learning_rate(lr, optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 0.97 every 2.4 epochs\"\"\"\n",
        "    lr = lr * (0.9 ** (epoch // 2))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def print_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(\"Epoch: [{}] Current learning rate (lr) = {}\".format(\n",
        "                                                    epoch, param_group['lr']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEBDPl1uElTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(md):\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    # classes = (\n",
        "    # 'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
        "    # 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
        "    # 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
        "    # 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
        "    # 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
        "    # 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    # 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    # 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    # 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    # 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "    # 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "    # 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "    # 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "    # 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
        "    # 'worm')\n",
        "\n",
        "    train_loader, test_loader = ImageProcessing()\n",
        "    \n",
        "    # print(len(classes))\n",
        "    # imageshow(train_loader, classes)\n",
        "\n",
        "    momentum = 0.9\n",
        "    epochs = 20\n",
        "    decay = 1e-5\n",
        "    lr = 0.0001\n",
        "\n",
        "    model = md.to(device)\n",
        "\n",
        "    summary(model,(3,224,224),batch_size=10)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "    \n",
        "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    optimizer = RAdam(model.parameters(),lr=lr,  eps=1e-5, weight_decay=decay)\n",
        "\n",
        "    # reduce_lr = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "    # reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print_learning_rate(optimizer,epoch+1)\n",
        "        training(model, train_loader, optimizer, criterion, epoch)\n",
        "        loss,_ = testing(model, test_loader, criterion)\n",
        "        adjust_learning_rate(lr, optimizer,epoch+1)\n",
        "        # reduce_lr.step()\n",
        "\n",
        "    PATH = './cifar_net.pth'\n",
        "    torch.save(model.state_dict(), PATH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohUgZtZ2IYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A formula differentiate the Swish operation\n",
        "class Swish(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.sigmoid(i)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_tensors[0]\n",
        "        sigmoid_i = torch.sigmoid(i)\n",
        "        return grad_output * (sigmoid_i + i * sigmoid_i * (1 - sigmoid_i))\n",
        "\n",
        "# MemoryEfficientSwish saves more memories than just using swish\n",
        "# Save the input for back propagation, save more space for computing derivative\n",
        "# https://medium.com/the-artificial-impostor/more-memory-efficient-swish-activation-function-e07c22c12a76\n",
        "class EfficientSwish(nn.Module):\n",
        "    def forward(self, input):\n",
        "        # S = Swish.apply\n",
        "        # return S(input)\n",
        "        return input * torch.sigmoid(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OdtW7qA8OzU",
        "colab_type": "code",
        "outputId": "cf3dc633-2547-4e72-cae6-9daed9b99172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(torch.hub.list('rwightman/gen-efficientnet-pytorch'))\n",
        "\n",
        "md = geffnet.create_model('efficientnet_b0',pretrained=True)\n",
        "\n",
        "md.classifier = torch.nn.Linear(1280,10,bias=True)\n",
        "\n",
        "# md.classifier = torch.nn.Sequential(\n",
        "#             torch.nn.Linear(1280,128,bias=True),\n",
        "#             nn.BatchNorm1d(num_features=128, eps=1e-5, momentum=1e-5),\n",
        "#             EfficientSwish(),\n",
        "#             nn.Dropout(0.5),\n",
        "\n",
        "#             # torch.nn.Linear(512,128,bias=True),\n",
        "#             # nn.BatchNorm1d(num_features=128, eps=1e-5, momentum=1e-5),\n",
        "#             # EfficientSwish(),\n",
        "#             # nn.Dropout(0.5),\n",
        "\n",
        "#             torch.nn.Linear(128,10,bias=True),\n",
        "#         )\n",
        "\n",
        "fine_tuning(md)\n",
        "\n",
        "# main(md)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_es', 'fbnetc_100', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_a1', 'mnasnet_b1', 'mobilenetv3_large_100', 'mobilenetv3_rw', 'spnasnet_100', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [10, 32, 112, 112]             864\n",
            "       BatchNorm2d-2         [10, 32, 112, 112]              64\n",
            "          SwishJit-3         [10, 32, 112, 112]               0\n",
            "            Conv2d-4         [10, 32, 112, 112]             288\n",
            "       BatchNorm2d-5         [10, 32, 112, 112]              64\n",
            "          SwishJit-6         [10, 32, 112, 112]               0\n",
            " AdaptiveAvgPool2d-7             [10, 32, 1, 1]               0\n",
            "            Conv2d-8              [10, 8, 1, 1]             264\n",
            "          SwishJit-9              [10, 8, 1, 1]               0\n",
            "           Conv2d-10             [10, 32, 1, 1]             288\n",
            "    SqueezeExcite-11         [10, 32, 112, 112]               0\n",
            "           Conv2d-12         [10, 16, 112, 112]             512\n",
            "      BatchNorm2d-13         [10, 16, 112, 112]              32\n",
            "         Identity-14         [10, 16, 112, 112]               0\n",
            "DepthwiseSeparableConv-15         [10, 16, 112, 112]               0\n",
            "           Conv2d-16         [10, 96, 112, 112]           1,536\n",
            "      BatchNorm2d-17         [10, 96, 112, 112]             192\n",
            "         SwishJit-18         [10, 96, 112, 112]               0\n",
            "           Conv2d-19           [10, 96, 56, 56]             864\n",
            "      BatchNorm2d-20           [10, 96, 56, 56]             192\n",
            "         SwishJit-21           [10, 96, 56, 56]               0\n",
            "AdaptiveAvgPool2d-22             [10, 96, 1, 1]               0\n",
            "           Conv2d-23              [10, 4, 1, 1]             388\n",
            "         SwishJit-24              [10, 4, 1, 1]               0\n",
            "           Conv2d-25             [10, 96, 1, 1]             480\n",
            "    SqueezeExcite-26           [10, 96, 56, 56]               0\n",
            "           Conv2d-27           [10, 24, 56, 56]           2,304\n",
            "      BatchNorm2d-28           [10, 24, 56, 56]              48\n",
            " InvertedResidual-29           [10, 24, 56, 56]               0\n",
            "           Conv2d-30          [10, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-31          [10, 144, 56, 56]             288\n",
            "         SwishJit-32          [10, 144, 56, 56]               0\n",
            "           Conv2d-33          [10, 144, 56, 56]           1,296\n",
            "      BatchNorm2d-34          [10, 144, 56, 56]             288\n",
            "         SwishJit-35          [10, 144, 56, 56]               0\n",
            "AdaptiveAvgPool2d-36            [10, 144, 1, 1]               0\n",
            "           Conv2d-37              [10, 6, 1, 1]             870\n",
            "         SwishJit-38              [10, 6, 1, 1]               0\n",
            "           Conv2d-39            [10, 144, 1, 1]           1,008\n",
            "    SqueezeExcite-40          [10, 144, 56, 56]               0\n",
            "           Conv2d-41           [10, 24, 56, 56]           3,456\n",
            "      BatchNorm2d-42           [10, 24, 56, 56]              48\n",
            " InvertedResidual-43           [10, 24, 56, 56]               0\n",
            "           Conv2d-44          [10, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-45          [10, 144, 56, 56]             288\n",
            "         SwishJit-46          [10, 144, 56, 56]               0\n",
            "           Conv2d-47          [10, 144, 28, 28]           3,600\n",
            "      BatchNorm2d-48          [10, 144, 28, 28]             288\n",
            "         SwishJit-49          [10, 144, 28, 28]               0\n",
            "AdaptiveAvgPool2d-50            [10, 144, 1, 1]               0\n",
            "           Conv2d-51              [10, 6, 1, 1]             870\n",
            "         SwishJit-52              [10, 6, 1, 1]               0\n",
            "           Conv2d-53            [10, 144, 1, 1]           1,008\n",
            "    SqueezeExcite-54          [10, 144, 28, 28]               0\n",
            "           Conv2d-55           [10, 40, 28, 28]           5,760\n",
            "      BatchNorm2d-56           [10, 40, 28, 28]              80\n",
            " InvertedResidual-57           [10, 40, 28, 28]               0\n",
            "           Conv2d-58          [10, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-59          [10, 240, 28, 28]             480\n",
            "         SwishJit-60          [10, 240, 28, 28]               0\n",
            "           Conv2d-61          [10, 240, 28, 28]           6,000\n",
            "      BatchNorm2d-62          [10, 240, 28, 28]             480\n",
            "         SwishJit-63          [10, 240, 28, 28]               0\n",
            "AdaptiveAvgPool2d-64            [10, 240, 1, 1]               0\n",
            "           Conv2d-65             [10, 10, 1, 1]           2,410\n",
            "         SwishJit-66             [10, 10, 1, 1]               0\n",
            "           Conv2d-67            [10, 240, 1, 1]           2,640\n",
            "    SqueezeExcite-68          [10, 240, 28, 28]               0\n",
            "           Conv2d-69           [10, 40, 28, 28]           9,600\n",
            "      BatchNorm2d-70           [10, 40, 28, 28]              80\n",
            " InvertedResidual-71           [10, 40, 28, 28]               0\n",
            "           Conv2d-72          [10, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-73          [10, 240, 28, 28]             480\n",
            "         SwishJit-74          [10, 240, 28, 28]               0\n",
            "           Conv2d-75          [10, 240, 14, 14]           2,160\n",
            "      BatchNorm2d-76          [10, 240, 14, 14]             480\n",
            "         SwishJit-77          [10, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-78            [10, 240, 1, 1]               0\n",
            "           Conv2d-79             [10, 10, 1, 1]           2,410\n",
            "         SwishJit-80             [10, 10, 1, 1]               0\n",
            "           Conv2d-81            [10, 240, 1, 1]           2,640\n",
            "    SqueezeExcite-82          [10, 240, 14, 14]               0\n",
            "           Conv2d-83           [10, 80, 14, 14]          19,200\n",
            "      BatchNorm2d-84           [10, 80, 14, 14]             160\n",
            " InvertedResidual-85           [10, 80, 14, 14]               0\n",
            "           Conv2d-86          [10, 480, 14, 14]          38,400\n",
            "      BatchNorm2d-87          [10, 480, 14, 14]             960\n",
            "         SwishJit-88          [10, 480, 14, 14]               0\n",
            "           Conv2d-89          [10, 480, 14, 14]           4,320\n",
            "      BatchNorm2d-90          [10, 480, 14, 14]             960\n",
            "         SwishJit-91          [10, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-92            [10, 480, 1, 1]               0\n",
            "           Conv2d-93             [10, 20, 1, 1]           9,620\n",
            "         SwishJit-94             [10, 20, 1, 1]               0\n",
            "           Conv2d-95            [10, 480, 1, 1]          10,080\n",
            "    SqueezeExcite-96          [10, 480, 14, 14]               0\n",
            "           Conv2d-97           [10, 80, 14, 14]          38,400\n",
            "      BatchNorm2d-98           [10, 80, 14, 14]             160\n",
            " InvertedResidual-99           [10, 80, 14, 14]               0\n",
            "          Conv2d-100          [10, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-101          [10, 480, 14, 14]             960\n",
            "        SwishJit-102          [10, 480, 14, 14]               0\n",
            "          Conv2d-103          [10, 480, 14, 14]           4,320\n",
            "     BatchNorm2d-104          [10, 480, 14, 14]             960\n",
            "        SwishJit-105          [10, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-106            [10, 480, 1, 1]               0\n",
            "          Conv2d-107             [10, 20, 1, 1]           9,620\n",
            "        SwishJit-108             [10, 20, 1, 1]               0\n",
            "          Conv2d-109            [10, 480, 1, 1]          10,080\n",
            "   SqueezeExcite-110          [10, 480, 14, 14]               0\n",
            "          Conv2d-111           [10, 80, 14, 14]          38,400\n",
            "     BatchNorm2d-112           [10, 80, 14, 14]             160\n",
            "InvertedResidual-113           [10, 80, 14, 14]               0\n",
            "          Conv2d-114          [10, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-115          [10, 480, 14, 14]             960\n",
            "        SwishJit-116          [10, 480, 14, 14]               0\n",
            "          Conv2d-117          [10, 480, 14, 14]          12,000\n",
            "     BatchNorm2d-118          [10, 480, 14, 14]             960\n",
            "        SwishJit-119          [10, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-120            [10, 480, 1, 1]               0\n",
            "          Conv2d-121             [10, 20, 1, 1]           9,620\n",
            "        SwishJit-122             [10, 20, 1, 1]               0\n",
            "          Conv2d-123            [10, 480, 1, 1]          10,080\n",
            "   SqueezeExcite-124          [10, 480, 14, 14]               0\n",
            "          Conv2d-125          [10, 112, 14, 14]          53,760\n",
            "     BatchNorm2d-126          [10, 112, 14, 14]             224\n",
            "InvertedResidual-127          [10, 112, 14, 14]               0\n",
            "          Conv2d-128          [10, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-129          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-130          [10, 672, 14, 14]               0\n",
            "          Conv2d-131          [10, 672, 14, 14]          16,800\n",
            "     BatchNorm2d-132          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-133          [10, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-134            [10, 672, 1, 1]               0\n",
            "          Conv2d-135             [10, 28, 1, 1]          18,844\n",
            "        SwishJit-136             [10, 28, 1, 1]               0\n",
            "          Conv2d-137            [10, 672, 1, 1]          19,488\n",
            "   SqueezeExcite-138          [10, 672, 14, 14]               0\n",
            "          Conv2d-139          [10, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-140          [10, 112, 14, 14]             224\n",
            "InvertedResidual-141          [10, 112, 14, 14]               0\n",
            "          Conv2d-142          [10, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-143          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-144          [10, 672, 14, 14]               0\n",
            "          Conv2d-145          [10, 672, 14, 14]          16,800\n",
            "     BatchNorm2d-146          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-147          [10, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-148            [10, 672, 1, 1]               0\n",
            "          Conv2d-149             [10, 28, 1, 1]          18,844\n",
            "        SwishJit-150             [10, 28, 1, 1]               0\n",
            "          Conv2d-151            [10, 672, 1, 1]          19,488\n",
            "   SqueezeExcite-152          [10, 672, 14, 14]               0\n",
            "          Conv2d-153          [10, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-154          [10, 112, 14, 14]             224\n",
            "InvertedResidual-155          [10, 112, 14, 14]               0\n",
            "          Conv2d-156          [10, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-157          [10, 672, 14, 14]           1,344\n",
            "        SwishJit-158          [10, 672, 14, 14]               0\n",
            "          Conv2d-159            [10, 672, 7, 7]          16,800\n",
            "     BatchNorm2d-160            [10, 672, 7, 7]           1,344\n",
            "        SwishJit-161            [10, 672, 7, 7]               0\n",
            "AdaptiveAvgPool2d-162            [10, 672, 1, 1]               0\n",
            "          Conv2d-163             [10, 28, 1, 1]          18,844\n",
            "        SwishJit-164             [10, 28, 1, 1]               0\n",
            "          Conv2d-165            [10, 672, 1, 1]          19,488\n",
            "   SqueezeExcite-166            [10, 672, 7, 7]               0\n",
            "          Conv2d-167            [10, 192, 7, 7]         129,024\n",
            "     BatchNorm2d-168            [10, 192, 7, 7]             384\n",
            "InvertedResidual-169            [10, 192, 7, 7]               0\n",
            "          Conv2d-170           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-171           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-172           [10, 1152, 7, 7]               0\n",
            "          Conv2d-173           [10, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-174           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-175           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-176           [10, 1152, 1, 1]               0\n",
            "          Conv2d-177             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-178             [10, 48, 1, 1]               0\n",
            "          Conv2d-179           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-180           [10, 1152, 7, 7]               0\n",
            "          Conv2d-181            [10, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-182            [10, 192, 7, 7]             384\n",
            "InvertedResidual-183            [10, 192, 7, 7]               0\n",
            "          Conv2d-184           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-185           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-186           [10, 1152, 7, 7]               0\n",
            "          Conv2d-187           [10, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-188           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-189           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-190           [10, 1152, 1, 1]               0\n",
            "          Conv2d-191             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-192             [10, 48, 1, 1]               0\n",
            "          Conv2d-193           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-194           [10, 1152, 7, 7]               0\n",
            "          Conv2d-195            [10, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-196            [10, 192, 7, 7]             384\n",
            "InvertedResidual-197            [10, 192, 7, 7]               0\n",
            "          Conv2d-198           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-199           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-200           [10, 1152, 7, 7]               0\n",
            "          Conv2d-201           [10, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-202           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-203           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-204           [10, 1152, 1, 1]               0\n",
            "          Conv2d-205             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-206             [10, 48, 1, 1]               0\n",
            "          Conv2d-207           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-208           [10, 1152, 7, 7]               0\n",
            "          Conv2d-209            [10, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-210            [10, 192, 7, 7]             384\n",
            "InvertedResidual-211            [10, 192, 7, 7]               0\n",
            "          Conv2d-212           [10, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-213           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-214           [10, 1152, 7, 7]               0\n",
            "          Conv2d-215           [10, 1152, 7, 7]          10,368\n",
            "     BatchNorm2d-216           [10, 1152, 7, 7]           2,304\n",
            "        SwishJit-217           [10, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-218           [10, 1152, 1, 1]               0\n",
            "          Conv2d-219             [10, 48, 1, 1]          55,344\n",
            "        SwishJit-220             [10, 48, 1, 1]               0\n",
            "          Conv2d-221           [10, 1152, 1, 1]          56,448\n",
            "   SqueezeExcite-222           [10, 1152, 7, 7]               0\n",
            "          Conv2d-223            [10, 320, 7, 7]         368,640\n",
            "     BatchNorm2d-224            [10, 320, 7, 7]             640\n",
            "InvertedResidual-225            [10, 320, 7, 7]               0\n",
            "          Conv2d-226           [10, 1280, 7, 7]         409,600\n",
            "     BatchNorm2d-227           [10, 1280, 7, 7]           2,560\n",
            "        SwishJit-228           [10, 1280, 7, 7]               0\n",
            "AdaptiveAvgPool2d-229           [10, 1280, 1, 1]               0\n",
            "          Linear-230                   [10, 10]          12,810\n",
            "================================================================\n",
            "Total params: 4,020,358\n",
            "Trainable params: 4,020,358\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 5.74\n",
            "Forward/backward pass size (MB): 1734.95\n",
            "Params size (MB): 15.34\n",
            "Estimated Total Size (MB): 1756.03\n",
            "----------------------------------------------------------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: [1] Current learning rate (lr) = 0.001\n",
            "[epoch: 1, batch:   200] loss: 1.1109\n",
            "[epoch: 1, batch:   400] loss: 0.4954\n",
            "[epoch: 1, batch:   600] loss: 0.3755\n",
            "[epoch: 1, batch:   800] loss: 0.3061\n",
            "[epoch: 1, batch:  1000] loss: 0.2519\n",
            "[epoch: 1, batch:  1200] loss: 0.2390\n",
            "[epoch: 1, batch:  1400] loss: 0.2132\n",
            "\n",
            "Test set: Average loss: 0.1725, Accuracy: 0.9427\n",
            "\n",
            "Epoch: [2] Current learning rate (lr) = 0.001\n",
            "[epoch: 2, batch:   200] loss: 0.1695\n",
            "[epoch: 2, batch:   400] loss: 0.1260\n",
            "[epoch: 2, batch:   600] loss: 0.1167\n",
            "[epoch: 2, batch:   800] loss: 0.0972\n",
            "[epoch: 2, batch:  1000] loss: 0.0825\n",
            "[epoch: 2, batch:  1200] loss: 0.0782\n",
            "[epoch: 2, batch:  1400] loss: 0.0705\n",
            "\n",
            "Test set: Average loss: 0.1590, Accuracy: 0.9514\n",
            "\n",
            "Epoch: [3] Current learning rate (lr) = 0.001\n",
            "[epoch: 3, batch:   200] loss: 0.0545\n",
            "[epoch: 3, batch:   400] loss: 0.0480\n",
            "[epoch: 3, batch:   600] loss: 0.0450\n",
            "[epoch: 3, batch:   800] loss: 0.0360\n",
            "[epoch: 3, batch:  1000] loss: 0.0334\n",
            "[epoch: 3, batch:  1200] loss: 0.0427\n",
            "[epoch: 3, batch:  1400] loss: 0.0330\n",
            "\n",
            "Test set: Average loss: 0.1677, Accuracy: 0.9542\n",
            "\n",
            "Epoch: [4] Current learning rate (lr) = 0.001\n",
            "[epoch: 4, batch:   200] loss: 0.0268\n",
            "[epoch: 4, batch:   400] loss: 0.0201\n",
            "[epoch: 4, batch:   600] loss: 0.0244\n",
            "[epoch: 4, batch:   800] loss: 0.0198\n",
            "[epoch: 4, batch:  1000] loss: 0.0166\n",
            "[epoch: 4, batch:  1200] loss: 0.0180\n",
            "[epoch: 4, batch:  1400] loss: 0.0176\n",
            "\n",
            "Test set: Average loss: 0.1764, Accuracy: 0.9546\n",
            "\n",
            "Epoch: [5] Current learning rate (lr) = 0.0005\n",
            "[epoch: 5, batch:   200] loss: 0.0132\n",
            "[epoch: 5, batch:   400] loss: 0.0094\n",
            "[epoch: 5, batch:   600] loss: 0.0067\n",
            "[epoch: 5, batch:   800] loss: 0.0082\n",
            "[epoch: 5, batch:  1000] loss: 0.0058\n",
            "[epoch: 5, batch:  1200] loss: 0.0036\n",
            "[epoch: 5, batch:  1400] loss: 0.0021\n",
            "\n",
            "Test set: Average loss: 0.1420, Accuracy: 0.9654\n",
            "\n",
            "Epoch: [6] Current learning rate (lr) = 0.0005\n",
            "[epoch: 6, batch:   200] loss: 0.0014\n",
            "[epoch: 6, batch:   400] loss: 0.0011\n",
            "[epoch: 6, batch:   600] loss: 0.0011\n",
            "[epoch: 6, batch:   800] loss: 0.0010\n",
            "[epoch: 6, batch:  1000] loss: 0.0006\n",
            "[epoch: 6, batch:  1200] loss: 0.0005\n",
            "[epoch: 6, batch:  1400] loss: 0.0004\n",
            "\n",
            "Test set: Average loss: 0.1407, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [7] Current learning rate (lr) = 0.0005\n",
            "[epoch: 7, batch:   200] loss: 0.0004\n",
            "[epoch: 7, batch:   400] loss: 0.0004\n",
            "[epoch: 7, batch:   600] loss: 0.0004\n",
            "[epoch: 7, batch:   800] loss: 0.0004\n",
            "[epoch: 7, batch:  1000] loss: 0.0003\n",
            "[epoch: 7, batch:  1200] loss: 0.0003\n",
            "[epoch: 7, batch:  1400] loss: 0.0002\n",
            "\n",
            "Test set: Average loss: 0.1421, Accuracy: 0.9669\n",
            "\n",
            "Epoch: [8] Current learning rate (lr) = 0.0005\n",
            "[epoch: 8, batch:   200] loss: 0.0003\n",
            "[epoch: 8, batch:   400] loss: 0.0003\n",
            "[epoch: 8, batch:   600] loss: 0.0003\n",
            "[epoch: 8, batch:   800] loss: 0.0003\n",
            "[epoch: 8, batch:  1000] loss: 0.0003\n",
            "[epoch: 8, batch:  1200] loss: 0.0002\n",
            "[epoch: 8, batch:  1400] loss: 0.0002\n",
            "\n",
            "Test set: Average loss: 0.1436, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [9] Current learning rate (lr) = 0.00025\n",
            "[epoch: 9, batch:   200] loss: 0.0002\n",
            "[epoch: 9, batch:   400] loss: 0.0002\n",
            "[epoch: 9, batch:   600] loss: 0.0003\n",
            "[epoch: 9, batch:   800] loss: 0.0002\n",
            "[epoch: 9, batch:  1000] loss: 0.0002\n",
            "[epoch: 9, batch:  1200] loss: 0.0002\n",
            "[epoch: 9, batch:  1400] loss: 0.0002\n",
            "\n",
            "Test set: Average loss: 0.1442, Accuracy: 0.9672\n",
            "\n",
            "Epoch: [10] Current learning rate (lr) = 0.00025\n",
            "[epoch: 10, batch:   200] loss: 0.0002\n",
            "[epoch: 10, batch:   400] loss: 0.0002\n",
            "[epoch: 10, batch:   600] loss: 0.0002\n",
            "[epoch: 10, batch:   800] loss: 0.0002\n",
            "[epoch: 10, batch:  1000] loss: 0.0002\n",
            "[epoch: 10, batch:  1200] loss: 0.0002\n",
            "[epoch: 10, batch:  1400] loss: 0.0002\n",
            "\n",
            "Test set: Average loss: 0.1447, Accuracy: 0.9673\n",
            "\n",
            "Epoch: [11] Current learning rate (lr) = 0.000125\n",
            "[epoch: 11, batch:   200] loss: 0.0002\n",
            "[epoch: 11, batch:   400] loss: 0.0002\n",
            "[epoch: 11, batch:   600] loss: 0.0002\n",
            "[epoch: 11, batch:   800] loss: 0.0002\n",
            "[epoch: 11, batch:  1000] loss: 0.0002\n",
            "[epoch: 11, batch:  1200] loss: 0.0002\n",
            "[epoch: 11, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1450, Accuracy: 0.9672\n",
            "\n",
            "Epoch: [12] Current learning rate (lr) = 0.000125\n",
            "[epoch: 12, batch:   200] loss: 0.0002\n",
            "[epoch: 12, batch:   400] loss: 0.0002\n",
            "[epoch: 12, batch:   600] loss: 0.0002\n",
            "[epoch: 12, batch:   800] loss: 0.0002\n",
            "[epoch: 12, batch:  1000] loss: 0.0002\n",
            "[epoch: 12, batch:  1200] loss: 0.0002\n",
            "[epoch: 12, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1453, Accuracy: 0.9672\n",
            "\n",
            "Epoch: [13] Current learning rate (lr) = 6.25e-05\n",
            "[epoch: 13, batch:   200] loss: 0.0002\n",
            "[epoch: 13, batch:   400] loss: 0.0002\n",
            "[epoch: 13, batch:   600] loss: 0.0002\n",
            "[epoch: 13, batch:   800] loss: 0.0002\n",
            "[epoch: 13, batch:  1000] loss: 0.0002\n",
            "[epoch: 13, batch:  1200] loss: 0.0002\n",
            "[epoch: 13, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1454, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [14] Current learning rate (lr) = 6.25e-05\n",
            "[epoch: 14, batch:   200] loss: 0.0002\n",
            "[epoch: 14, batch:   400] loss: 0.0002\n",
            "[epoch: 14, batch:   600] loss: 0.0002\n",
            "[epoch: 14, batch:   800] loss: 0.0002\n",
            "[epoch: 14, batch:  1000] loss: 0.0002\n",
            "[epoch: 14, batch:  1200] loss: 0.0002\n",
            "[epoch: 14, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1455, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [15] Current learning rate (lr) = 3.125e-05\n",
            "[epoch: 15, batch:   200] loss: 0.0002\n",
            "[epoch: 15, batch:   400] loss: 0.0002\n",
            "[epoch: 15, batch:   600] loss: 0.0002\n",
            "[epoch: 15, batch:   800] loss: 0.0002\n",
            "[epoch: 15, batch:  1000] loss: 0.0002\n",
            "[epoch: 15, batch:  1200] loss: 0.0002\n",
            "[epoch: 15, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1456, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [16] Current learning rate (lr) = 3.125e-05\n",
            "[epoch: 16, batch:   200] loss: 0.0002\n",
            "[epoch: 16, batch:   400] loss: 0.0002\n",
            "[epoch: 16, batch:   600] loss: 0.0002\n",
            "[epoch: 16, batch:   800] loss: 0.0002\n",
            "[epoch: 16, batch:  1000] loss: 0.0002\n",
            "[epoch: 16, batch:  1200] loss: 0.0002\n",
            "[epoch: 16, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1457, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [17] Current learning rate (lr) = 1.5625e-05\n",
            "[epoch: 17, batch:   200] loss: 0.0002\n",
            "[epoch: 17, batch:   400] loss: 0.0002\n",
            "[epoch: 17, batch:   600] loss: 0.0002\n",
            "[epoch: 17, batch:   800] loss: 0.0002\n",
            "[epoch: 17, batch:  1000] loss: 0.0002\n",
            "[epoch: 17, batch:  1200] loss: 0.0002\n",
            "[epoch: 17, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1457, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [18] Current learning rate (lr) = 1.5625e-05\n",
            "[epoch: 18, batch:   200] loss: 0.0002\n",
            "[epoch: 18, batch:   400] loss: 0.0002\n",
            "[epoch: 18, batch:   600] loss: 0.0002\n",
            "[epoch: 18, batch:   800] loss: 0.0002\n",
            "[epoch: 18, batch:  1000] loss: 0.0002\n",
            "[epoch: 18, batch:  1200] loss: 0.0002\n",
            "[epoch: 18, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1457, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [19] Current learning rate (lr) = 7.8125e-06\n",
            "[epoch: 19, batch:   200] loss: 0.0002\n",
            "[epoch: 19, batch:   400] loss: 0.0002\n",
            "[epoch: 19, batch:   600] loss: 0.0002\n",
            "[epoch: 19, batch:   800] loss: 0.0002\n",
            "[epoch: 19, batch:  1000] loss: 0.0002\n",
            "[epoch: 19, batch:  1200] loss: 0.0002\n",
            "[epoch: 19, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1457, Accuracy: 0.9671\n",
            "\n",
            "Epoch: [20] Current learning rate (lr) = 7.8125e-06\n",
            "[epoch: 20, batch:   200] loss: 0.0002\n",
            "[epoch: 20, batch:   400] loss: 0.0002\n",
            "[epoch: 20, batch:   600] loss: 0.0002\n",
            "[epoch: 20, batch:   800] loss: 0.0002\n",
            "[epoch: 20, batch:  1000] loss: 0.0002\n",
            "[epoch: 20, batch:  1200] loss: 0.0002\n",
            "[epoch: 20, batch:  1400] loss: 0.0001\n",
            "\n",
            "Test set: Average loss: 0.1458, Accuracy: 0.9671\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAADeCAYAAADy3YFwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenEfficientNet(\n",
              "  (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act1): SwishJit()\n",
              "  (blocks): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): DepthwiseSeparableConv(\n",
              "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): SwishJit()\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act2): SwishJit()\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SwishJit()\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act2): SwishJit()\n",
              "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Linear(in_features=1280, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODoDGptnnEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "test_pred = md.predict(x_test)\n",
        "ax = sns.heatmap(confusion_matrix(np.argmax(y_test, axis=1),np.argmax(test_pred, axis=1)), cmap=\"YlGnBu\",annot=True,fmt=\"d\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfValnVg2Vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWvZzqkAKRLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "['efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_es', 'fbnetc_100', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_a1', 'mnasnet_b1', 'mobilenetv3_large_100', 'mobilenetv3_rw', 'spnasnet_100', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100']\n",
        "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
        "Files already downloaded and verified\n",
        "Files already downloaded and verified\n",
        "Epoch: [1] Current learning rate (lr) = 0.0001\n",
        "[epoch: 1, batch:   200] loss: 1.3310\n",
        "[epoch: 1, batch:   400] loss: 0.6354\n",
        "[epoch: 1, batch:   600] loss: 0.4598\n",
        "[epoch: 1, batch:   800] loss: 0.3937\n",
        "[epoch: 1, batch:  1000] loss: 0.3280\n",
        "[epoch: 1, batch:  1200] loss: 0.3127\n",
        "[epoch: 1, batch:  1400] loss: 0.2587\n",
        "\n",
        "Test set: Average loss: 0.8596, Accuracy: 0.9535\n",
        "\n",
        "Epoch: [2] Current learning rate (lr) = 0.0001\n",
        "[epoch: 2, batch:   200] loss: 0.2190\n",
        "[epoch: 2, batch:   400] loss: 0.1774\n",
        "[epoch: 2, batch:   600] loss: 0.1314\n",
        "[epoch: 2, batch:   800] loss: 0.1113\n",
        "[epoch: 2, batch:  1000] loss: 0.0891\n",
        "[epoch: 2, batch:  1200] loss: 0.0843\n",
        "[epoch: 2, batch:  1400] loss: 0.0788\n",
        "\n",
        "Test set: Average loss: 0.4884, Accuracy: 0.9558\n",
        "\n",
        "Epoch: [3] Current learning rate (lr) = 0.0001\n",
        "[epoch: 3, batch:   200] loss: 0.0700\n",
        "[epoch: 3, batch:   400] loss: 0.0511\n",
        "[epoch: 3, batch:   600] loss: 0.0430\n",
        "[epoch: 3, batch:   800] loss: 0.0382\n",
        "[epoch: 3, batch:  1000] loss: 0.0363\n",
        "[epoch: 3, batch:  1200] loss: 0.0404\n",
        "[epoch: 3, batch:  1400] loss: 0.0333\n",
        "\n",
        "Test set: Average loss: 0.2801, Accuracy: 0.9494\n",
        "\n",
        "Epoch: [4] Current learning rate (lr) = 0.0001\n",
        "[epoch: 4, batch:   200] loss: 0.0432\n",
        "[epoch: 4, batch:   400] loss: 0.0293\n",
        "[epoch: 4, batch:   600] loss: 0.0334\n",
        "[epoch: 4, batch:   800] loss: 0.0340\n",
        "[epoch: 4, batch:  1000] loss: 0.0378\n",
        "[epoch: 4, batch:  1200] loss: 0.0325\n",
        "[epoch: 4, batch:  1400] loss: 0.0356\n",
        "\n",
        "Test set: Average loss: 0.1869, Accuracy: 0.9566\n",
        "\n",
        "Epoch: [5] Current learning rate (lr) = 0.0001\n",
        "[epoch: 5, batch:   200] loss: 0.0236\n",
        "[epoch: 5, batch:   400] loss: 0.0266\n",
        "[epoch: 5, batch:   600] loss: 0.0207\n",
        "[epoch: 5, batch:   800] loss: 0.0208\n",
        "[epoch: 5, batch:  1000] loss: 0.0176\n",
        "[epoch: 5, batch:  1200] loss: 0.0133\n",
        "[epoch: 5, batch:  1400] loss: 0.0202\n",
        "\n",
        "Test set: Average loss: 0.1556, Accuracy: 0.9566\n",
        "\n",
        "Epoch: [6] Current learning rate (lr) = 0.0001\n",
        "[epoch: 6, batch:   200] loss: 0.0217\n",
        "[epoch: 6, batch:   400] loss: 0.0257\n",
        "[epoch: 6, batch:   600] loss: 0.0248\n",
        "[epoch: 6, batch:   800] loss: 0.0187\n",
        "[epoch: 6, batch:  1000] loss: 0.0171\n",
        "[epoch: 6, batch:  1200] loss: 0.0158\n",
        "[epoch: 6, batch:  1400] loss: 0.0147\n",
        "\n",
        "Test set: Average loss: 0.1556, Accuracy: 0.9563\n",
        "\n",
        "Epoch: [7] Current learning rate (lr) = 0.0001\n",
        "[epoch: 7, batch:   200] loss: 0.0179\n",
        "[epoch: 7, batch:   400] loss: 0.0222\n",
        "[epoch: 7, batch:   600] loss: 0.0234\n",
        "[epoch: 7, batch:   800] loss: 0.0187\n",
        "[epoch: 7, batch:  1000] loss: 0.0210\n",
        "[epoch: 7, batch:  1200] loss: 0.0185\n",
        "[epoch: 7, batch:  1400] loss: 0.0135\n",
        "\n",
        "Test set: Average loss: 0.1658, Accuracy: 0.9567\n",
        "\n",
        "Epoch: [8] Current learning rate (lr) = 0.0001\n",
        "[epoch: 8, batch:   200] loss: 0.0102\n",
        "[epoch: 8, batch:   400] loss: 0.0127\n",
        "[epoch: 8, batch:   600] loss: 0.0158\n",
        "[epoch: 8, batch:   800] loss: 0.0245\n",
        "[epoch: 8, batch:  1000] loss: 0.0228\n",
        "[epoch: 8, batch:  1200] loss: 0.0216\n",
        "[epoch: 8, batch:  1400] loss: 0.0157\n",
        "\n",
        "Test set: Average loss: 0.1692, Accuracy: 0.9586\n",
        "\n",
        "Epoch: [9] Current learning rate (lr) = 5e-05\n",
        "[epoch: 9, batch:   200] loss: 0.0115\n",
        "[epoch: 9, batch:   400] loss: 0.0089\n",
        "[epoch: 9, batch:   600] loss: 0.0056\n",
        "[epoch: 9, batch:   800] loss: 0.0075\n",
        "[epoch: 9, batch:  1000] loss: 0.0051\n",
        "[epoch: 9, batch:  1200] loss: 0.0035\n",
        "[epoch: 9, batch:  1400] loss: 0.0025\n",
        "\n",
        "Test set: Average loss: 0.1493, Accuracy: 0.9652\n",
        "\n",
        "Epoch: [10] Current learning rate (lr) = 5e-05\n",
        "[epoch: 10, batch:   200] loss: 0.0020\n",
        "[epoch: 10, batch:   400] loss: 0.0025\n",
        "[epoch: 10, batch:   600] loss: 0.0016\n",
        "[epoch: 10, batch:   800] loss: 0.0017\n",
        "[epoch: 10, batch:  1000] loss: 0.0011\n",
        "[epoch: 10, batch:  1200] loss: 0.0017\n",
        "[epoch: 10, batch:  1400] loss: 0.0007\n",
        "\n",
        "Test set: Average loss: 0.1536, Accuracy: 0.9655\n",
        "\n",
        "Epoch: [11] Current learning rate (lr) = 5e-05\n",
        "[epoch: 11, batch:   200] loss: 0.0007\n",
        "[epoch: 11, batch:   400] loss: 0.0007\n",
        "[epoch: 11, batch:   600] loss: 0.0005\n",
        "[epoch: 11, batch:   800] loss: 0.0006\n",
        "[epoch: 11, batch:  1000] loss: 0.0006\n",
        "[epoch: 11, batch:  1200] loss: 0.0005\n",
        "[epoch: 11, batch:  1400] loss: 0.0003\n",
        "\n",
        "Test set: Average loss: 0.1512, Accuracy: 0.9663\n",
        "\n",
        "Epoch: [12] Current learning rate (lr) = 2.5e-05\n",
        "[epoch: 12, batch:   200] loss: 0.0004\n",
        "[epoch: 12, batch:   400] loss: 0.0004\n",
        "[epoch: 12, batch:   600] loss: 0.0003\n",
        "[epoch: 12, batch:   800] loss: 0.0003\n",
        "[epoch: 12, batch:  1000] loss: 0.0003\n",
        "[epoch: 12, batch:  1200] loss: 0.0003\n",
        "[epoch: 12, batch:  1400] loss: 0.0002\n",
        "\n",
        "Test set: Average loss: 0.1505, Accuracy: 0.9678\n",
        "\n",
        "Epoch: [13] Current learning rate (lr) = 2.5e-05\n",
        "[epoch: 13, batch:   200] loss: 0.0003\n",
        "[epoch: 13, batch:   400] loss: 0.0003\n",
        "[epoch: 13, batch:   600] loss: 0.0002\n",
        "[epoch: 13, batch:   800] loss: 0.0002\n",
        "[epoch: 13, batch:  1000] loss: 0.0002\n",
        "[epoch: 13, batch:  1200] loss: 0.0002\n",
        "[epoch: 13, batch:  1400] loss: 0.0002\n",
        "\n",
        "Test set: Average loss: 0.1508, Accuracy: 0.9674\n",
        "\n",
        "Epoch: [14] Current learning rate (lr) = 1.25e-05\n",
        "[epoch: 14, batch:   200] loss: 0.0002\n",
        "[epoch: 14, batch:   400] loss: 0.0002\n",
        "[epoch: 14, batch:   600] loss: 0.0002\n",
        "[epoch: 14, batch:   800] loss: 0.0002\n",
        "[epoch: 14, batch:  1000] loss: 0.0002\n",
        "[epoch: 14, batch:  1200] loss: 0.0002\n",
        "[epoch: 14, batch:  1400] loss: 0.0001\n",
        "\n",
        "Test set: Average loss: 0.1502, Accuracy: 0.9681\n",
        "\n",
        "Epoch: [15] Current learning rate (lr) = 1.25e-05\n",
        "[epoch: 15, batch:   200] loss: 0.0002\n",
        "[epoch: 15, batch:   400] loss: 0.0002\n",
        "[epoch: 15, batch:   600] loss: 0.0001\n",
        "[epoch: 15, batch:   800] loss: 0.0001\n",
        "[epoch: 15, batch:  1000] loss: 0.0002\n",
        "[epoch: 15, batch:  1200] loss: 0.0001\n",
        "[epoch: 15, batch:  1400] loss: 0.0001\n",
        "\n",
        "Test set: Average loss: 0.1494, Accuracy: 0.9680\n",
        "\n",
        "Epoch: [16] Current learning rate (lr) = 6.25e-06\n",
        "[epoch: 16, batch:   200] loss: 0.0001\n",
        "[epoch: 16, batch:   400] loss: 0.0001\n",
        "[epoch: 16, batch:   600] loss: 0.0001\n",
        "[epoch: 16, batch:   800] loss: 0.0001\n",
        "[epoch: 16, batch:  1000] loss: 0.0001\n",
        "[epoch: 16, batch:  1200] loss: 0.0001\n",
        "[epoch: 16, batch:  1400] loss: 0.0001\n",
        "\n",
        "Test set: Average loss: 0.1486, Accuracy: 0.9682\n",
        "\n",
        "Epoch: [17] Current learning rate (lr) = 6.25e-06\n",
        "[epoch: 17, batch:   200] loss: 0.0001\n",
        "[epoch: 17, batch:   400] loss: 0.0001\n",
        "[epoch: 17, batch:   600] loss: 0.0001\n",
        "[epoch: 17, batch:   800] loss: 0.0001\n",
        "[epoch: 17, batch:  1000] loss: 0.0001\n",
        "[epoch: 17, batch:  1200] loss: 0.0001\n",
        "[epoch: 17, batch:  1400] loss: 0.0001\n",
        "\n",
        "Test set: Average loss: 0.1485, Accuracy: 0.9679\n",
        "\n",
        "Epoch: [18] Current learning rate (lr) = 6.25e-06\n",
        "[epoch: 18, batch:   200] loss: 0.0001\n",
        "[epoch: 18, batch:   400] loss: 0.0001\n",
        "[epoch: 18, batch:   600] loss: 0.0001\n",
        "[epoch: 18, batch:   800] loss: 0.0001\n",
        "[epoch: 18, batch:  1000] loss: 0.0001\n",
        "[epoch: 18, batch:  1200] loss: 0.0001\n",
        "[epoch: 18, batch:  1400] loss: 0.0001\n",
        "\n",
        "Test set: Average loss: 0.1478, Accuracy: 0.9678\n",
        "\n",
        "Epoch: [19] Current learning rate (lr) = 6.25e-06\n",
        "[epoch: 19, batch:   200] loss: 0.0001\n",
        "[epoch: 19, batch:   400] loss: 0.0001\n",
        "[epoch: 19, batch:   600] loss: 0.0001\n",
        "[epoch: 19, batch:   800] loss: 0.0001\n",
        "[epoch: 19, batch:  1000] loss: 0.0001\n",
        "[epoch: 19, batch:  1200] loss: 0.0001\n",
        "[epoch: 19, batch:  1400] loss: 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}